---
title: "Describing vocalizations in young children: A big data approach through citizen science annotation"
shorttitle        : "Children's vocalizations and citizen science"

author: 
  - name          : "Chiara Semenzin"
    affiliation   : "1"
    corresponding : no
  - name          : "Lisa Hamrick"
    affiliation   : "2"
    corresponding : no
  - name          : "Amanda Seidl"
    affiliation   : "2"
    corresponding : no
  - name          : "Bridgette L. Kelleher"
    affiliation   : "2"
    corresponding : no
  - name          : "Alejandrina Cristia"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "29 rue d'Ulm, 75005, Paris, France"
    email         : "alecristia@gmail.com"

affiliation:
  - id            : "1"
    institution   : "Laboratoire de Sciences Cognitives et de Psycholinguistique, Département d'Etudes cognitives, ENS, EHESS, CNRS, PSL University"
  - id            : "2"
    institution   : "Purdue University"    
    
author_note: >
       AC acknowledges Agence Nationale de la Recherche (ANR-17-CE28-0007 LangAge, ANR-16-DATA-0004 ACLEW, ANR-14-CE30-0003 MechELex, ANR-17-EURE-0017) and the J. S. McDonnell Foundation (Understanding Human Cognition Scholar Award); LH the National Institute of Deafness and Other Communication Disorders (F31DC018219); BK  the National Institute of Mental Health (K23MH111955) and the Kinley Trust. This publication uses data generated via the Zooniverse.org platform, development of which is funded by generous support, including a Global Impact Award from Google, and by a grant from the Alfred P. Sloan Foundation. The funders had no impact on this study.  All authors approved the final manuscript as submitted, agree to be accountable for all aspects of the work and have no conflict of interests to disclose.

       
# Abbreviations LENA&reg;: Language Environment Analysis.
# ASHA recommends that abstracts be 150–250 words. The size limit for what can be included in your submission is set above 300 words,
#currently 259 words

abstract: >
  **Purpose:** Recording young children's vocalizations through wearables is a promising method to assess language development. However, accurately and rapidly annotating these files remains challenging. Online crowdsourcing with the collaboration of citizen scientists could be a feasible solution. In this paper, we assess the extent to which citizen scientists' annotations align with those gathered in the lab for recordings collected from young children. 
  \newline
  
  
  **Method:** Segments identified by LENA&reg; as produced by the key child were extracted from one daylong recording for each of 20 participants: 10  low-risk control children and 10 children diagnosed  with  Angelman  syndrome, a  neurogenetic  syndrome  characterized  by  severe  language  impairments. Speech samples were annotated by trained annotators in the laboratory as well as by citizen scientists on Zooniverse. All annotators assigned one of five labels to each sample: Canonical, Non-Canonical, Crying, Laughing, and Junk. This allowed the derivation of two child-level vocalization metrics: the Linguistic Proportion, and the Canonical Proportion.

  \newline
 
  
  **Results:** At the segment level, Zooniverse classifications had moderate precision and recall. More importantly, the Linguistic Proportion and the Canonical Proportion derived from Zooniverse annotations were highly correlated with those derived from laboratory annotations.

  \newline
 
  
  **Conclusion:** Annotations obtained through a citizen science platform can help us overcome challenges posed by the process of annotating daylong speech recordings. Particularly when used in composites or derived metrics, such annotations can be used to investigate early markers of language delays.
 
  
# #up to 3 keywords
# keywords          : "reliability; validation; daylong audiorecordings; citizen science; Zooniverse; LENA; long-form recordings"
# wordcount         : "9375"

bibliography      : ["zoo.bib"]

figsintext        : no
figurelist        : yes
tablelist         : no
footnotelist      : no
lineno            : no

always_allow_html: true

header-includes:
  - \usepackage{setspace}
  - \AtBeginEnvironment{tabular}{\singlespacing}
  - \AtBeginEnvironment{lltable}{\singlespacing}
  - \AtBeginEnvironment{tablenotes}{\singlespacing}
  - \captionsetup[table]{font={stretch=1.5}}
  - \captionsetup[figure]{font={stretch=1.5}}
  
class             : "man"
output            : papaja::apa6_pdf

---

```{r setup, include = FALSE}


library(groundhog)
groundhog.day="2021-02-23"
#groundhog.library('papaja', groundhog.day) #NOTE! THIS IS A POINT OF POTENTIAL NON-REPRODUCIBILITY
library(papaja) #NOTE!
groundhog.library('tidyverse', groundhog.day)
groundhog.library('dplyr', groundhog.day)
groundhog.library('ggplot2', groundhog.day)
groundhog.library('rmarkdown', groundhog.day)
groundhog.library('caret', groundhog.day)
groundhog.library('scales', groundhog.day)
groundhog.library('kableExtra', groundhog.day)
groundhog.library('rel', groundhog.day)
groundhog.library('ggpubr', groundhog.day)
groundhog.library('lme4', groundhog.day)
groundhog.library('e1071', groundhog.day)

#knitr::opts_chunk$set(warning = FALSE, message = FALSE, cache = FALSE, fig.pos = "T")
knitr::opts_chunk$set(warning = FALSE, message = FALSE, cache = FALSE)
```

```{r analysis-preferences, include = FALSE}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)

#document session info
capture.output(sessionInfo(),file="paper-lastknit_session_info.txt")

printr=function(x) sub("0.", ".", round(x,3))

```

```{r read-in}

read.csv("key_info.csv")->x
rownames(x)<-x$X


read.csv(".chunks_maj_judgments.csv")->chunks

read.csv("zoo_lab_maj_judgments.csv")->data_all


label_options=c("Canonical" , "Non-Canonical"  ,     "Crying"   ,    "Laughing",        "Junk" )

#use better names
data_all$Zoon_classif=data_all$Answer

# create lab column with easier to read correspondance
data_all$lab<-as.character(data_all$Major_Choice)
data_all$lab[data_all$lab=="Non-canonical syllables"]<-"Non-Canonical"
data_all$lab[data_all$lab=="Canonical syllables"]<-"Canonical"
data_all$lab[data_all$lab %in% c("Don't mark","None")]<-"Junk"
data_all$lab=factor(data_all$lab,levels=label_options)
#apply same factor levels as zooniverse so that we can do symmetrical confusion matrices

#add binomials for Linguistic Proportion
data_all$lab_ling=ifelse(data_all$lab %in% c("Canonical","Non-Canonical"),1,0)
data_all$zoo_ling=ifelse(data_all$Zoon_classif %in% c("Canonical","Non-Canonical"),1,0)
data_all$lab_ling[data_all$lab=="Junk"]<-NA
data_all$zoo_ling[data_all$lab=="Junk"]<-NA

#add binomials for Canonical Proportion
data_all$lab_can=data_all$zoo_can=NA
data_all$lab_can[data_all$lab=="Canonical"]<-1
data_all$lab_can[data_all$lab=="Non-Canonical"]<-0
data_all$zoo_can[data_all$Zoon_classif=="Canonical"]<-1
data_all$zoo_can[data_all$Zoon_classif=="Non-Canonical"]<-0


demo_data=read.csv("demo-data.tsv",sep ="\t")
#add filenames to demo data, to be used later
demo_data_fn <- demo_data %>% 
     left_join(select(data_all, filename, ChildID), by = c("ChildID"))
demo_data_fn<-unique(demo_data_fn)

```



Since early language delays can adversely affect children’s literacy, behavior, social interaction, and scholastic achievement extending well into adulthood, early interventions have been described as a better societal investment than later ones [@heckman2006skill]. Some research suggests that spontaneous behavior, captured for instance via home videos, could provide important indices to development [@belardi2017retrospective;@overby2020retrospective]. Advancements in the field of wearable technologies, such as LENA&reg; recorders, have opened new avenues to both early detection of speech pathologies and research in language development more generally [@rankine2017language;@vandam2019use;@oller2010automated]. Wearable recorders allow data collection to happen in the child’s natural environment, and at a large scale, which may be particularly helpful for children whose speech is not easily elicited. Although such long-form recordings are increasingly common [@ganek2018language], challenges remain with respect to how these data are handled, annotated, and analyzed [@casillas2019step]. The present work reports on the validity of labels and child-level descriptors of children's vocalizations gathered from citizen scientists, in a comparison to expert lab annotators. 

In the rest of the Introduction, we first briefly summarize two metrics that can be used to describe individual children's vocalizations in the context of long-form recordings. We then introduce crowdsourcing in general, and crowdsourcing by citizen scientists in particular, as a potential avenue for more rapidly deriving these key metrics from vocalization recordings. 

###  Describing children's vocalizations in long-form recordings

A large body of research has investigated both fine-grained and coarse descriptions of children's vocalizations as a function of age and diagnosis. It is beyond the scope of this paper to provide a full summary of this extensive body work, mainly because the way in which vocal development has been studied in the past may not easily translate to long-form recordings. Long-form recordings have several advantages, including capturing the child's vocal patterns in their natural environment and being able to accumulate a great amount of data easily, which is particularly useful for diagnoses and pathologies characterized by low levels of vocal production. However, this also means that the audiorecording is harder to process than an audiorecording gathered in more manicured and stable conditions: In a typical daylong recording, the child will sometimes be in a quiet room with just her primary caregiver, but later in a noisy supermarket or having dinner with family and friends. In fact, human annotation of such audio is estimated to take about 30 times the audio length, with these estimates being greater the more precise one wants or needs to be [@casillas2017new]. It is for this reason that most users of long-form recordings have turned to automatized software for at least a first-pass analysis.

#### Automated first-pass analyses

The most commonly used software was created by the LENA&reg; Foundation, and it returns a diarization of the audio signal split into key talkers, including the child wearing the device, as well as a split of sections attributed to the child into three categories: crying, other fixed and vegetative signals, and speech vocalizations. These subcategories have not been widely validated [@cristia2020accuracy].  Moreover, until recently LENA&reg; lacked an important distinction between more and less advanced vocalizations.[^1]

[^1]: The latest version of the LENA&reg; software does tag canonical utterances separately from non-canonical ones.

Other automatized algorithms have been developed in the last two years, saliently ones attempting to classify child vocalizations into crying, laughing, canonical, and non-canonical [@schuller2019interspeech]. The overt inclusion of laughing allows a somewhat finer-grained picture of children's vocal activity than what the LENA&reg; algorithm currently offers. Another advantage of this new line of research is that there exists a challenge where performance has been benchmarked, so unlike LENA&reg;'s algorithms, we know which algorithms perform better than others. Indeed, the ComParE 2019 BabySounds sub-challenge established a state-of-the-art baseline with an unweighted average recall of around 55% on the test set. By re-using the same test set, researchers can develop new algorithms and prove that they can outperform this baseline (or any subsequent best-performing algorithm benchmarked in the same test set). 

Table 1 provides an idea of the accuracy across different methods. Specifically, we calculated accuracy of the LENA&reg;'s subtypes[^2] based on the laboratory annotations used in the present study; and we also show the results of ComParE 2019 BabySounds sub-challenge, although note it is not based on the same data as the other two columns. Table 1 shows that LENA&reg; has a good recall for speech-like vocalizations and crying but not laughing, and at the time it did not distinguish between canonical and non-canonical vocalizations. The ComParE 2019 BabySounds sub-challenge's baseline model achieved a more moderate recall on crying than LENA, but clearly outperforms the LENA&reg; algorithm in the Laughing category, with the additional advantage of distinguishing canonical and non-canonical -- although note that performance in non-canonical is barely above a chance level of 20% (based on 5 categories, assuming equal probabilities). Notice that the baseline model also included a category called "Junk" for sections that turned out not to be the child's vocalizations after all. LENA&reg; has a low recall for the Junk category because only sections that LENA&reg; classified as being child vocalizations are considered in the present analysis. As a result, any segment that turns out not to be a child vocalization is a LENA&reg; error. We return to the comparison between LENA&reg;, the ComParE 2019 BabySounds sub-challenge baseline, and our Zooniverse-based method in the Discussion; for now, we simply point out that there are new algorithms being developed, but their performance --although competitive when compared to LENA&reg;-- is underwhelming. 


----- INSERT TABLE 1 HERE -----


[^2]: If a child segment contained speech, then it counted towards canonical/non-canonical; else, if it contained crying it counted towards crying; else, if it contained some fixed signals it counted towards laughing; a small proportion were left that did not have any of the three and were considered as "Junk" or not categorized. When these data were analyzed, the LENA&reg; software did not distinguish canonical and non-canonical.

#### Derived metrics

One possibility that has only recently begun to be explored is the use of *derived metrics*. So instead of evaluating algorithms and other annotation procedures on  accuracy of individual sections of the audio, one can integrate over time (e.g., over the whole day-long audio) to derive a metric that more closely relates to vocalization development. 

For example, most work using LENA&reg; software reports on children's vocalization counts -- a derived metric because it is not simply a description of sections of the audio, but instead it integrates over the whole recording length. Although linguistic vocalization count has been criticized as being more about quantity than quality [@mcdaniel2020predicting], it is a promising metric of individuals' vocalization development because it shows correlations with age (which is a proxy of development), and it can be extracted quite accurately with LENA&reg; [@cristia2020accuracy]. Additionally, the child vocalization count metric has been found to be concurrently and predictively correlated with an effect size r ~ .3 with standardized language scores in a meta-analysis [@wang2020meta]. LENA&reg;'s child vocalization counts (CVC) aggregates all key child vocalizations detected as being *linguistic*. As a result, an increase in CVC could indicate an actual increase in how many linguistic vocalizations the child produces, or a change in how well they are detected as such by the algorithm. Thus, despite representing a composite of skills, vocalization count may provide a useful estimate of vocalization development. 


Here, we focus on two alternative composite metrics. One metric that can be derived once children's vocalizations are split into crying, laughing, canonical, and non-canonical is the Linguistic Proportion: the proportion of vocalizations that are linguistic (canonical and non-canonical) out of all vocalizations. To our knowledge, this metric has not been extensively explored previously, but it is likely that this proportion increases with age early on and eventually plateaus.


Yet another metric that could be extracted to estimate relative linguistic complexity is the Canonical Proportion: the proportion of vocalizations that contain a canonical transition or syllable out of all linguistic (canonical and non-canonical) vocalizations. @cychosz2021canonical found that the Canonical Proportion extracted from daylong recordings was significantly correlated with age in a multicultural and multilingual sample of children up to 3 years of age, and thus well beyond the babbling period, and into first words and word combinations. That study built on a wider research base documenting the potential importance of the Canonical Proportion, and related metrics, although this prior work focuses on babbling and thus typically on infants under one year of age. In these younger infants, a critical milestone involves the increasingly common production of canonical syllables, consonant-vowel or vowel-consonant sequences that resemble those found in adult speech [@oller1998late]. Given its adult-like consonant-vowel or vowel-consonant structure, canonical babble is considered to be a starting point on the path to recognizable speech. When compared to more primitive sounds such as squeals, or isolated vowels, canonical syllables show a higher complexity given the smooth articulatory transition between a consonant and a vowel (or vice versa). Some work suggests that Canonical Proportions above .15 are expected by about 10 months of age in typical development [@oller2000emergence;@cychosz2021canonical]. In addition, the proportion of vocalizations containing a canonical syllable has been found to be more predictive of individual development than sheer vocalization counts in a sample of children diagnosed with an autism spectrum disorder [@mcdaniel2020predicting].





### Crowdsourcing: A potential solution for annotation

In sum, there is a growing literature attempting to use data from daylong recordings to describe young children's vocalizations, but there are two outstanding challenges. The first pertains to how the data are annotated, with human annotation being costly. The second relates to how descriptors of vocal development are generated, i.e., how annotations of individual audio sections are integrated over all data for a given child to derive child-level vocal development metrics.


"Crowdsourcing" refers to the process whereby a task is solved by a crowd, rather than an individual. A number of fields, particularly in the data-driven sciences, have already engaged in the collection of data (including annotations) through crowdsourcing, thanks to its low cost and ecological value [@crump2013evaluating;@sescleifer2018systematic]. For example, a systematic review on crowdsourced ratings of speech found that "lay ratings are highly concordant with expert opinion, validating crowdsourcing as a reliable methodology"; across studies, crowdsourced and expert listener classifications yielded a mean correlation coefficient of .81 [@sescleifer2018systematic]. On the other hand, the systematic review returned only 8 studies (of which only four were published in peer-reviewed journals), suggesting that there is considerable need for further research on this topic.

Mechanical Turk (MTurk) MTurk is an online labor market created by Amazon to assist "requesters" in hiring and paying "workers" for the completion of computerized tasks. Although it is a leading crowdsourcing service, and some evidence suggests MTurkers' annotations can be quite reliable [@berinsky2012evaluating],  question marks are raised as some "workers" turn out to be bots, or are poorly motivated (and potentially exploited) humans.

#### Citizen scientists to the rescue

A promising crowd-sourcing alternative has arisen in recent years: citizen science, a research technique that engages the public in the collection of scientific information. As citizen scientists do not receive compensation, this alternative to platforms such as MTurk can overcome the limitations posed by potential exploitation of workforce and/or the use of bots. Volunteers are entirely motivated by the desire to contribute to research advancements as well as the pleasure they derive from the task itself.

One of the most successful platforms hosting citizen science projects is Zooniverse [zooniverse.org; @borne2011zooniverse]. The website hosts a multitude of interdisciplinary projects that have allowed the public to take part in cutting-edge scientific research, from marine biology to papyrology. Zooniverse has proven extremely useful in those fields where the complexity of the data collected is too high to be automatically interpreted using computer algorithms. At the same time, the tasks that human volunteers are asked to complete are sufficiently simple that citizens can carry them out without a background in science or any extensive training.

Citizen science may be particularly helpful when analyzing infants' data from wearables, which remain challenging to annotate as mentioned above. There is one previous study that attempted this approach. @cychosz2021canonical drew child vocalization data from a diverse set of corpora centered on children learning one of four languages: English, Tseltal, Tsimane', and Yélî. For the English and Tsimane' corpora, vocalizations were automatically identified using LENA&reg;, whereas the other two were extracted through manual segmentation. Segments were then split into maximally 500-ms long clips, and presented to annotators through the citizen science iHearUPlay platform, with the aim that each clip received three classifications into Crying, Laughing, Canonical, Non-Canonical, or Junk (with the latter tag used for clips that did not contain a child voice). They then derived an implementation of the Canonical Proportion, as the proportion of clips receiving a majority judgment of canonical out of the clips receiving a majority judgment of canonical or non-canonical. They found that this Canonical Proportion increased with infant age in an analysis collapsing across corpora, which provides a first proof of principle that citizen scientists' annotations could be an appropriate solution to the problem of annotating child vocalizations from wearables. The authors also studied how their Canonical Proportion related to age within each corpus that had a sufficient number of children varying in age. They found that in one English corpus bearing on very young infants, the proportion of clips assigned to the "Junk" category was very high. Additionally, they found that the correlation between Canonical Proportion and age was much weaker in two corpora where vocalizations were segmented with LENA&reg; than through manual segmentation. In sum, these analyses within corpora reveal that data may not be uniformly useful, with potential variation across corpora, children, and methods. 


### The present work

Our work seeks to broaden the already promising results that have emerged from previous work summarized above. We hope that this methodology will open the road to larger scale analyses of children's vocalizations as captured by wearables. In addition, we went beyond @cychosz2021canonical's study in two important ways. First, we relied on the largest and best established citizen science platform: Zooniverse hosts more than 1.6 million users from diverse walks of life, and it offers a completely automatized API system to more easily scale tasks in a transparent and cumulative science fashion. In contrast, @cychosz2021canonical actively recruited people to provide annotations, stating that "[a]nnotators included language and speech researchers, undergraduate students, research assistants, and other interested parties, totaling 136 unique annotators". A process that draws from research assistants relies on expert resources, whose time is expensive. Moreover, the people these researchers recruit tend to be more accustomed to child vocalization data, which may improve their annotation performance, and thus provide an overestimate of the quality of the annotations that can be realistically done within a truly citizen science framework. Second, a core goal of the present work was to determine how citizen scientists' annotations fare compared to the current gold standard, laboratory annotations. @cychosz2021canonical did not have a gold standard for their data. 

Thus, the present study aimed to contribute a key piece of evidence missing in this discussion: To what extent do laboratory and citizen science annotations agree when describing young children's vocalizations? We examined the extent to which such classifications agree, by quantifying the correspondence across these two modes of annotations at the level of individual segments, and at the level of individual children. 

At the segment level, we checked the extent to which laboratory annotations made by experts agreed with judgments made by citizen scientists. We used confusion matrices to describe annotation convergence and divergence patterns, and overall accuracy, kappa, and Gwet's AC1 coefficient as statistical descriptors. Given previous results suggesting that performing this classification based on local acoustic cues is challenging [@Seidl19;@schuller2019interspeech], we expected agreement to be only moderate at this level.

When considering correspondence at the level of individual children, we derived two metrics: the Linguistic Proportion (Canonical+Non-Canonical)/(all non-Junk labels); and the Canonical Proportion (Canonical)/(all linguistic labels). 

It was important for generalizability purposes to examine data from children with variable biological age, mental age, and vocal maturity levels. We therefore included a group of low-risk control infants, who are close to the population most commonly sampled in studies using LENA&reg; [see meta-analyses in @wang2020meta;@cristia2020accuracy]; as well as children who had been diagnosed with Angelman syndrome and whose ages spanned a wide age range. Some previous work had shown that vocal development proceeded differently among children with an Angelman syndrome diagnosis when compared to a normative sample [@hamrick2019capturing;@hamrick2019measurement]. Generally, studies on vocal development among children with genetic disorders reveal lower volubility and lower prevalence of canonical syllables [@rankine2017language;@belardi2017retrospective; see also @roche2018early for a review of retrospective video analysis of children diagnosed with Autism Spectrum Disorder, Rett Syndrome, or Fragile X, documenting these and other vocal characteristics]. Low volubility is one of the contexts in which daylong recordings may be particularly advantageous over alternative recording methods [see @rankine2017language for discussion], since it is challenging to measure vocal development accurately based on shorter recordings (as the child may not vocalize at all in a 1-2h recording). Additionally, the ideal method is sensitive not only to the large changes that occur over normative development, but also to the potentially smaller individual differences found among children. Such a level of sensitivity is important if these methods are ever to be used to detect potential effects of intervention.

Returning to our overall predictions with these goals in mind, we were particularly interested in describing potential differences in accuracy of derived metrics as a function of our two participant groups. Previous work suggests that children with Angelman syndrome show decreases in Canonical Proportion with age, whereas in typically-developing infants, conversely, the Canonical Proportion was expected to increase with age [@hamrick2019capturing;@hamrick2019measurement]. Although we could not rely on previous work to make predictions regarding the Linguistic Proportion, we reasoned that we should observe an increase in the Linguistic Proportion for both populations. More specific to our research aims, we checked the degree to which laboratory and citizen science annotations converged at the child level by using correlations across the two. 

## Methods

All analyses and key data are available for reproduction on OSF (https://osf.io/57yha/). Computer code used to collect the Zooniverse judgments is available from GitHub [@semenzin2020zooniverse]. This manuscript is reproducible thanks to the use of RMd [@baumer2015r] and Papaja [@aust2017papaja]  on R [@team2013r].

### Participants 

The full data set includes data from 20 children: 10 English-speaking children (6 males, 4 females; age M = 41.5 months, range 11-53 months) diagnosed with Angelman syndrome and 10 low-risk control children (6 males, 4 females; age M = 11.7 months, range 4-18 months). As children diagnosed with Angelman syndrome typically have severe cognitive and language delays and most do not produce more than 1-2 words consistently, we compare their linguistic production data against that of younger children with theoretically similar language profiles. This study was approved by the local institutional review board under the project name Neurodevelopmental Natural History Study, Purdue University, IRB-1811021381.

### Equipment and data collection procedure

Recordings were obtained with the Language ENvironment Analysis (LENA&reg;) Digital Language Processor,  a lightweight recorder (< 60 g, 5.5 cm x 8.5 cm x 1.5 cm) designed to be worn inside a breast-pocket of purpose-made clothing. In our data, every child contributed at least one daylong recording, with one child contributing two recordings (duration 11.58-16.00 hours, mean 15.05 hours).

### Data preprocessing (LENA&reg;)			

The first step we took before beginning the annotation process was the identification of children's vocalizations within day-long recordings. We did this automatically, using the LENA&reg; proprietary software [@xu2008signal], with versions ranging from LENA Pro 3.5 to LENA HUB 3.3 to 4.0. In broad terms, the LENA&reg; software attempts to assign segments of speech either to the key child (the one wearing the recorder), or to one of 15 other categories in the child’s environment (e.g., Female-Adult, TV). Key child labels have a precision of about 60% and a recall of about 50% [@cristia2020thorough]. These stretches of audio assigned to the key child can contain cries, vegetative sounds, and linguistic vocalizations, as well as interstitial pauses. We refer to them as *segments*, to highlight the fact that they are stretches of audio segmented out as belonging to the key child, but they may not be individual vocalizations.



### Annotation by lab experts

Annotation was carried out at the segment level during lab coding. A subset of segments LENA&reg; identified as the key child were selected for human annotation.  Segments were selected from 30 five-minute sections of the child’s recording: 10 sections during which the child was vocalizing at their highest rate, and 20 sections drawn randomly from the remainder of the recording. To ensure generalizability of our data, we included in this subset a number of segments of audio coming from both periods of high child volubility, as well as randomly selected periods of the day.

Annotations were carried out by 20 undergraduate students working in either Speech, Language, and Hearing Sciences or Psychological Science laboratories. Before annotation, coders completed an ethical conduct of research course and received HIPAA compliance training. After this, they completed a brief training on vocal maturity that involved 1) reading relevant literature, 2) reviewing examples of various annotation categories and 3) submitting answers to a brief quiz to test their annotation accuracy. Coders then proceeded to the classification, where they made decisions on whether a segment was a cry, laugh, non-canonical syllable(s), canonical syllable(s), or a word. Coders were instructed to classify each segment based on the highest level of vocal maturity it contained (e.g., a segment with both canonical and non-canonical syllables would be classified as canonical; a segment with both laughing and non-canonical syllables would be classified as non-canonical). For canonical, non-canonical, and word segments, coders indicated how many syllables of each type of vocal maturity they heard (e.g., "ah ma ba" would be 1 non-canonical and 2 canonical syllables). For the purposes of the present study, we considered a "word" judgment to be equivalent to a "canonical" judgment. This may be false sometimes (e.g., "oh" may be coded as a real word by laboratory experts, although it is a non-canonical syllable), but we could not have a "word" judgment in our Zooniverse annotations because that requires context (which compromises privacy -- see next section). 

Coders were also given a "Don’t Mark" option, which they were instructed to use if the segment did not sound like a segment made by the target child, or if there is any overlapping speech or other noise in the background which could affect acoustic analyses. This category is broader than the "Junk" category used by Zooniverse annotators, which is explained in the next section. Training materials for laboratory coders are available from @hamrick2021si.

This process resulted in high levels of inter-rater agreement: Two or more of the three total coders who annotated that segment agreed on `r round(sum(data_all$Num_Agreement>=2)/dim(data_all)[1]*100,1)`% of the lab-annotated segments.



### Annotation on Zooniverse

A substantial concern that emerges when considering citizen science annotation for spontaneous speech data is the risk of a privacy breach: Even short clips can contain personal information that can expose the identity of the speaker, such as addresses or names, or contain sensitive and private information. Fortunately, @Seidl19 determined that when segments are divided into shorter clips (400-600 ms), human annotators with little training can code our categories of interest (Canonical, Laughing, Crying, etc.) with a classification quality comparable to the one carried out on full segments. We therefore used very short clips (500 ms) as these are unlikely to contain more than two syllables, and thus prevent the identification of any personal information. 

Specifically, the segments extracted were automatically cut into clips of exactly 500 ms, extracting neighboring silence when necessary, before being uploaded on the Zooniverse platform. We call this level *chunk*, to convey that they are part of a bigger unit (the LENA&reg;-identified "segment"). Note that segments and chunks are collectively referred to as *clips* because they are parts of the longer audio recording. To allow the recovery of the original segments at later stages of the analysis, the scripts created a metadata file with the mapping between 500-ms chunks and their corresponding segment. Chunks were then uploaded on Zooniverse using Panoptes, their open-source, command-line based API for data handling [@bowyer2015panoptes].

Citizen scientists could access our project, "Maturity of Baby Sounds", by logging in, or as anonymous annotators. Before starting the annotation, citizen scientists were given a quick tutorial which walked them through the steps of the annotation workflow (i.e., how to play a sound, how to make a selection; see Figure 1). In this tutorial, we included one audio example for each category in order to make the classification task as smooth as possible. Further clarifications on what constitutes canonical and non-canonical sequences, as well as five audio examples for each of the categories, were available through the Field Guide, which users could access by clicking on the right side of the screen. The Field Guide could be consulted at any point of the classification without interrupting the task.

----- INSERT FIGURE 1 HERE -----


Users were asked to assign each 500-ms chunk to one out of five possible categories: (1) Canonical, (2) Non-Canonical, (3) Crying, (4) Laughing, and (5) Junk (overlapping speech, non-infant speech, silence, external sounds). Notice that the latter is narrower than the "Don't mark" category of the lab annotation routine, which was used any time there was any noise or overlap that could affect acoustic analyses. 



Following recommendations from the Zooniverse board, we collected five judgments per clip, rather than three as used in the laboratory.  Majority agreement is thus achieved when three out of five judgments agree (rather than two out of three). 




### Data post-processing


The following description uses several key concepts: A "judgment" is a single classification provided by a Zooniverse user in response to a single chunk (or a single classification provided by a lab annotator in response to a single segment); a "chunk" is a 500-ms clip extracted from the daylong audio to represent part or all of a segment; a segment is a section of the daylong audio automatically classified as having been uttered by the key child according to the LENA&reg; algorithm; and a "label" is the final category a chunk or segment was classified as. Bearing this terminology in mind, we provide an overview of the data and how it was processed before the main analyses.

An impressive total of `r prettyNum(x["nusers","x"],big.mark=",")` individual Zooniverse users provided `r prettyNum(x["njudgments","x"],big.mark=",")` judgments on `r prettyNum(x["nchunks","x"],big.mark=",")` 500-ms chunks, corresponding to `r prettyNum(dim(data_all)[1],big.mark=",")` LENA&reg; segments. Given that each chunk is 500 ms in length, this means about `r round((x["nchunks","x"]*.5)/60/60,2)` hours of audio data were annotated by 8 different annotators (3 in the laboratory, 5 on Zooniverse). For a variety of technical mishaps, `r prettyNum((x["njudgments","x"] - dim(chunks)[1]*5),big.mark=",")` judgments had to be excluded, leaving `r prettyNum(length(levels(factor(chunks$filename))),big.mark=",")` chunks for which we had five Zooniverse judgments (for a total of `r prettyNum(dim(chunks)[1]*5,big.mark=",")` judgments included). Among these `r prettyNum(length(levels(factor(chunks$filename))),big.mark=",")` chunks, `r round(sum(chunks$nvotes<2)/dim(chunks)[1]*100,2)`% had no matching judgments, `r round(sum(chunks$nvotes==2)/dim(chunks)[1]*100,2)`% had two matching judgments (the threshold used for lab-annotated segments), and `r round(sum(chunks$nvotes>=3)/dim(chunks)[1]*100,2)`% had three or more matching judgments, or majority agreement within Zooniverse judgments. Future work may explore different ways of setting the minimal requirement for convergence, but for further analyses here, we focused on the `r round(sum(chunks$nvotes>=3)/dim(chunks)[1]*100,2)`%  of chunks that *did* have at least three judgments in agreement (i.e., majority agreement). This represented `r prettyNum(sum(chunks$nvotes>=3)*5,big.mark=",")` judgments for `r prettyNum(sum(chunks$nvotes>=3),big.mark=",")` chunks. 



```{r clean up, echo=F}
#dim(data_all) #11980    16
# remove non-majority labels from lab
data_all[data_all$Num_Agreement>0,]->data_all
#dim(data_all) # 11647    16

# remove non-majority labels from zooniverse
data_all[data_all$Zoon_classif %in% label_options,]->data_all
#dim(data_all) # 11268    16


#and reset the factors for cleanliness
data_all$Zoon_classif=factor(data_all$Zoon_classif, levels=label_options)
data_all$lab=factor(data_all$lab, levels=label_options)
sample_data<-cbind(data_all$lab,data_all$Zoon_classif)


table_zoon=round(table(data_all$Zoon)/sum(table(data_all$Zoon))*100,2)
table_lab=round(table(data_all$lab)/sum(table(data_all$lab))*100,2)
```


There was one more layer of processing that was necessary for our main analyses: Lab annotators provided judgments at the level of LENA&reg; segments, whereas Zooniverse annotators provided judgments on 500 ms chunks, which are typically smaller than the segments. Therefore, we needed to combine chunk-level labels to reconstruct segment-level labels. We did this by considering the majority labels of all chunks associated with each segment as follows: 

- If the majority label for all chunks associated with a given segment were Junk, then the segment was labelled as Junk,  
- else, if one Canonical judgment was present for any of the chunks associated to the segment, then the segment was labelled as Canonical, 
- else, if a Non-Canonical judgment was present, the segment was labelled as Non-Canonical, 
- else, if a Crying judgment was present, the segment was labelled as Crying,
- else, the segment was labelled as Laughing.

This is essentially the same thought process Lab annotators followed when jduging segments: a LENA&reg; segment may contain 1 or more vocalizations, which may be of the same type or not. Figure 2 conveys the relationship between chunks and segments, and how judgments are combined into labels. After removing segments for which there was no majority label in the lab and/or Zooniverse coding, we were left with data for `r dim(data_all)[1]` segments.

----- INSERT FIGURE 2 HERE -----





```{r prepare data by child, echo=F}

#get the ns by child, then calculate the linguistic proportion & canonical proportion, separately for zooniverse & lab
ztab=table(data_all$filename,data_all$Zoon_classif)
z_lp =rowSums(ztab[,c("Canonical","Non-Canonical")])/rowSums(ztab[,-which(colnames(ztab) %in% c("Junk"))])
z_cp =ztab[,c("Canonical")]/rowSums(ztab[,c("Canonical","Non-Canonical")])

ltab=table(data_all$filename,data_all$lab)
l_lp =rowSums(ltab[,c("Canonical","Non-Canonical")])/rowSums(ltab[,-which(colnames(ztab) %in% c("Junk"))])
l_cp =ltab[,c("Canonical")]/rowSums(ltab[,c("Canonical","Non-Canonical")])

#and also get Junk from zooniverse
z_Junk=ztab[,c("Junk")]/rowSums(ztab)

  
#put all the proportions together
if(sum(rownames(ztab)==rownames(ltab))==dim(ztab)[1]) proportions=cbind(rownames(ztab),z_lp,z_cp,l_lp,l_cp,z_Junk) else print("oops this code needs to be more complex because we don't have the same kids for the two proportions")
colnames(proportions)[1]<-"filename"

#ages=aggregate(data_all$Age,by=list(data_all$ChildID),mean) #this is a weird way of adding ages, since all of the ages for a given child should be the same if there is only one recording, and if there are multiple recordings, then we should not get the mean
#improvement: now we merge with a demo data tab, but note this is merged with child id, so the problem of multiple recs per child is still there

# Created demo_data with filenames. Use filenames instead of childIDs to merge proportions and demo data.

merge(proportions,demo_data_fn,by="filename")->proportions
colnames(proportions)[dim(proportions)[2]]<-"Age"

#cbinding results in text, so we numerize the proportions
for(thisvar in c("z_lp","z_cp","l_lp","l_cp","z_Junk")) proportions[,thisvar]=as.numeric(as.character(proportions[,thisvar]))

#add median Junk
proportions$median_Junk<-ifelse(proportions$z_Junk<median(proportions$z_Junk),"lower_Junk","higher_Junk")

#summary(proportions)

merge(data_all,proportions,all=T)->data_all
```
## Results

### Descriptive analyses



In this section, we provide descriptive analyses of our dataset. According to lab annotators, `r table_lab["Canonical"]`% of segments were Canonical, `r table_lab["Non-Canonical"]`% Non-Canonical, `r table_lab["Laughing"]`% Laughing, and `r table_lab["Crying"]`% Crying, with the remaining `r table_lab["Junk"]`% being categorized as "Don't mark". Zooniverse data revealed a similar distribution:  `r table_zoon["Canonical"]`% Canonical, `r table_zoon["Non-Canonical"]`% Non-Canonical, `r table_zoon["Laughing"]`% Laughing, `r table_zoon["Crying"]`% Crying, `r table_zoon["Junk"]`% Junk.
Next, we inspected the relationship between age and child-level derived metrics, of which we had two: i) Linguistic Proportion = (Canonical + Non-Canonical) / All vocalizations (i.e., we remove Junk), and ii) Canonical Proportion = Canonical / (Canonical + Non-Canonical) (i.e., we remove Junk, Crying, and Laughing). Figure 3 shows results of plotting our Proportions against child age. We draw the readers' attention to the fact that the children diagnosed with Angelman syndrome had overall higher levels of Linguistic Proportion than the typically-developing infants, but the opposite was true for Canonical Proportion.




```{r matr}
#create matrix to hold r results
myr=matrix(NA,nrow=4,ncol=2)
rownames(myr)<-c("z_lp","z_cp","l_lp","l_cp")
colnames(myr)<-c("AS","LR")
mylb=myhb=myn=myp =myr #this is an ugly way of creating 5 matrices

for(thisvar in c("z_lp","z_cp","l_lp","l_cp")) {


  #get cor properties
  thiscor=cor.test(proportions[proportions$Diagnosis=="AngelmanSyndrome",thisvar],proportions$Age[proportions$Diagnosis=="AngelmanSyndrome"])
    myr[thisvar,"AS"]=printr(thiscor$estimate)
    mylb[thisvar,"AS"]=round(thiscor$conf.int[1],3)
    myhb[thisvar,"AS"]=round(thiscor$conf.int[2],3)
    myn[thisvar,"AS"]=round(thiscor$parameter,3)
    myp[thisvar,"AS"]=round(thiscor$p.value,3)
    

  #repeat for LR  

      #get cor properties
  thiscor=cor.test(proportions[proportions$Diagnosis!="AngelmanSyndrome",thisvar],proportions$Age[proportions$Diagnosis!="AngelmanSyndrome"])
    myr[thisvar,"LR"]=printr(thiscor$estimate)
    mylb[thisvar,"LR"]=round(thiscor$conf.int[1],3)
    myhb[thisvar,"LR"]=round(thiscor$conf.int[2],3)
    myn[thisvar,"LR"]=round(thiscor$parameter,3)
    myp[thisvar,"LR"]=round(thiscor$p.value,3)

}

#matrix that will hold all r results
corage<-t(myr)[,c("l_lp","z_lp","l_cp","z_cp")]

```

----- INSERT FIGURE 3 HERE -----


Descriptive analyses on the laboratory annotations showed that correlations between the Linguistic Proportion and age differed across the groups. There was a near-zero relationship among the children diagnosed with Angelman syndrome r(`r myn["l_lp","AS"]`) = `r (myr["l_lp","AS"])`, CI [`r printr(mylb["l_lp","AS"])`,`r printr(myhb["l_lp","AS"])`], *p* = `r printp(myp["l_lp","AS"])`]; and a significant association among low-risk control children r(`r myn["l_lp","LR"]`) = `r (myr["l_lp","LR"])`, CI [`r printr(mylb["l_lp","LR"])`,`r printr(myhb["l_lp","LR"])`], *p* = `r printp(myp["l_lp","LR"])`]. The Canonical Proportion exhibited a non-significant developmental decrease among children diagnosed with Angelman syndrome r(`r myn["l_cp","AS"]`) = `r (myr["l_cp","AS"])`, CI [`r printr(mylb["l_cp","AS"])`,`r printr(myhb["l_cp","AS"])`], *p* = `r printp(myp["l_cp","AS"])`]; and a marginal developmental increase among low-risk control r(`r myn["l_cp","LR"]`) = `r (myr["l_cp","LR"])`, CI [`r printr(mylb["l_cp","LR"])`,`r printr(myhb["l_cp","LR"])`], *p* = `r printp(myp["l_cp","LR"])`].


Using the Zooniverse annotations, we found that the association with age was very weak for children diagnosed with Angelman syndrome r(`r myn["z_lp","AS"]`) = `r (myr["z_lp","AS"])`, CI [`r printr(mylb["z_lp","AS"])`,`r printr(myhb["z_lp","AS"])`], *p* = `r printp(myp["z_lp","AS"])`]; whereas low-risk control children showed a significant increase with age r(`r myn["z_lp","LR"]`) = `r (myr["z_lp","LR"])`, CI [`r printr(mylb["z_lp","LR"])`,`r printr(myhb["z_lp","LR"])`], *p* = `r printp(myp["z_lp","LR"])`]. Similarly, there was a non-significant developmental decrease in the Canonical Proportion among children with Angelman syndrome r(`r myn["z_cp","AS"]`) = `r (myr["z_cp","AS"])`, CI [`r printr(mylb["z_cp","AS"])`,`r printr(myhb["z_cp","AS"])`], *p* = `r printp(myp["z_cp","AS"])`]; and a marginal developmental increase among low-risk control children, r(`r myn["z_cp","LR"]`) = `r (myr["z_cp","LR"])`, CI [`r printr(mylb["z_cp","LR"])`,`r printr(myhb["z_cp","LR"])`], *p* = `r printp(myp["z_cp","LR"])`].



## Main analyses

Next, we discuss the correspondence between citizen science labels and the laboratory gold standard, at the level of individual segments. Results were visualized with a confusion matrix showing precision and recall (Figure 4): the diagonal elements show the number of correct segment-level labels for each class while the off-diagonal elements show non-matching labels.

```{r mygac-conf, echo=F}
mygac=gac(data = sample_data, kat = 5, weight = c("unweighted"),
    conf.level = 0.95)

mycf=confusionMatrix(data_all$lab, data_all$Zoon_classif, dnn = c("Lab","Zooniverse"))
conf_tab=mycf$table
# this package uses sensitivity & specificity
#Sensitivity=recall
#Specificity=precision
#mycf

#compose precision
colsums=colSums(conf_tab)
my_conf_tab=conf_tab
for(i in 1:dim(conf_tab)[2]) my_conf_tab[,i]=my_conf_tab[,i]/colsums[i]
#colSums(my_conf_tab) #check
prop_cat=data.frame(my_conf_tab*100) #generates precision because columns
prop_cat$id=paste(prop_cat$Lab,prop_cat$Zooniverse)
colnames(prop_cat)[3]<-"pr"
data.frame(conf_tab)->stall_p
stall_p$id=paste(stall_p$Lab,stall_p$Zooniverse)
stall_p=merge(stall_p,prop_cat[c("id","pr")])

# repeat for recall
rowsums=rowSums(conf_tab)
my_conf_tab=conf_tab
for(i in 1:dim(conf_tab)[2]) my_conf_tab[,i]=my_conf_tab[,i]/rowsums[i]
#rowSums(my_conf_tab)
prop_cat=data.frame(conf_tab/rowSums(conf_tab)*100)  #generates recall because rows
prop_cat$id=paste(prop_cat$Lab,prop_cat$Zooniverse)
colnames(prop_cat)[3]<-"rec"
data.frame(conf_tab)->stall_r
stall_r$id=paste(stall_r$Lab,stall_r$Zooniverse)
stall_r=merge(stall_r,prop_cat[c("id","rec")])
```



----- INSERT FIGURE 4 HERE -----



These visualizations suggest that performance was moderate to good, which was confirmed via statistical analyses. The overall (weighted) accuracy was `r round(mycf$overall["Accuracy"]*100)`%, CI = [`r round(mycf$overall["AccuracyLower"]*100)`,`r round(mycf$overall["AccuracyUpper"]*100)`],  kappa is `r printr(mycf$overall["Kappa"])`, and the Gwet's AC1 coefficient is `r printr(mygac$est)`, CI = [`r printr(mygac$lb)`,`r printr(mygac$ub)`]. 



### Child level descriptors

```{r matr2}
#create matrix to hold r results
myr=matrix(NA,nrow=2,ncol=3)
rownames(myr)<-c("lp","cp")
colnames(myr)<-c("AS","LR","all")
mylb=myhb=myn=myp =myr #this is an ugly way of creating 5 matrices

  #get cor properties
  thiscor=cor.test(proportions[proportions$Diagnosis=="AngelmanSyndrome","z_lp"],proportions[proportions$Diagnosis=="AngelmanSyndrome","l_lp"])
    myr[1,"AS"]=round(thiscor$estimate,3)
    mylb[1,"AS"]=round(thiscor$conf.int[1],3)
    myhb[1,"AS"]=round(thiscor$conf.int[2],3)
    myn[1,"AS"]=round(thiscor$parameter,3)
    myp[1,"AS"]=round(thiscor$p.value,3)
    

  #repeat for LR  

      #get cor properties
  thiscor=cor.test(proportions[proportions$Diagnosis!="AngelmanSyndrome","l_lp"],proportions[proportions$Diagnosis!="AngelmanSyndrome","z_lp"])
    myr[1,"LR"]=round(thiscor$estimate,3)
    mylb[1,"LR"]=round(thiscor$conf.int[1],3)
    myhb[1,"LR"]=round(thiscor$conf.int[2],3)
    myn[1,"LR"]=round(thiscor$parameter,3)
    myp[1,"LR"]=round(thiscor$p.value,3)
    
#repeat for all cor
    
      thiscor=cor.test(proportions$z_lp,proportions$l_lp)
    myr[1,"all"]=round(thiscor$estimate,3)
    mylb[1,"all"]=round(thiscor$conf.int[1],3)
    myhb[1,"all"]=round(thiscor$conf.int[2],3)
    myn[1,"all"]=round(thiscor$parameter,3)
    myp[1,"all"]=round(thiscor$p.value,3)
    
    # repeat whole for cp
    

  #get cor properties
  thiscor=cor.test(proportions[proportions$Diagnosis=="AngelmanSyndrome","z_cp"],proportions[proportions$Diagnosis=="AngelmanSyndrome","l_cp"])
    myr[2,"AS"]=round(thiscor$estimate,3)
    mylb[2,"AS"]=round(thiscor$conf.int[1],3)
    myhb[2,"AS"]=round(thiscor$conf.int[2],3)
    myn[2,"AS"]=round(thiscor$parameter,3)
    myp[2,"AS"]=round(thiscor$p.value,3)
    

  #repeat for LR  

      #get cor properties
  thiscor=cor.test(proportions[proportions$Diagnosis!="AngelmanSyndrome","l_cp"],proportions[proportions$Diagnosis!="AngelmanSyndrome","z_cp"])
    myr[2,"LR"]=round(thiscor$estimate,3)
    mylb[2,"LR"]=round(thiscor$conf.int[1],3)
    myhb[2,"LR"]=round(thiscor$conf.int[2],3)
    myn[2,"LR"]=round(thiscor$parameter,3)
    myp[2,"LR"]=round(thiscor$p.value,3)
    
#repeat for all cor
    
      thiscor=cor.test(proportions$z_cp,proportions$l_cp)
    myr[2,"all"]=round(thiscor$estimate,3)
    mylb[2,"all"]=round(thiscor$conf.int[1],3)
    myhb[2,"all"]=round(thiscor$conf.int[2],3)
    myn[2,"all"]=round(thiscor$parameter,3)
    myp[2,"all"]=round(thiscor$p.value,3)
    
    
mycortab=t(myr)
rownames(mycortab)<-paste("All seg",rownames(mycortab))


```


Although the classification at the segment level was only moderately accurate, what we are ultimately interested in is whether citizen scientists’ classifications are able to provide a reliable snapshot of children's individual development. We illustrate this association in Figure 5. Looking at all 20 children together, we found a strong positive correlation r(`r myn["lp","all"]`) = `r printr(myr["lp","all"])`, CI [`r printr(mylb["lp","all"])`,`r printr(myhb["lp","all"])`], *p* `r printp(myp["lp","all"])`] between Linguistic Proportion by child from the Zooniverse and the lab annotators' data.  When we split by participant group, correlations remained high: for Angelman syndrome r(`r myn["lp","AS"]`) = `r printr(myr["lp","AS"])`, CI [`r printr(mylb["lp","AS"])`,`r printr(myhb["lp","AS"])`], *p* = `r printp(myp["lp","AS"])`]; low-risk control r(`r myn["lp","LR"]`) = `r printr(myr["lp","LR"])`, CI [`r printr(mylb["lp","LR"])`,`r printr(myhb["lp","LR"])`], *p* = `r printp(myp["lp","LR"])`]. 

----- INSERT FIGURE 5 HERE -----

Similarly, a strong positive correlation was found in the Canonical Proportion r(`r myn["cp","all"]`) = `r printr(myr["cp","all"])`, CI [`r printr(mylb["cp","all"])`,`r printr(myhb["cp","all"])`], *p* `r printp(myp["cp","all"])`]. When we split by participant group, correlations remained high although we do note they were somewhat lower for the children with Angelman syndrome: r(`r myn["cp","AS"]`) = `r printr(myr["cp","AS"])`, CI [`r printr(mylb["cp","AS"])`,`r printr(myhb["cp","AS"])`], *p* = `r myp["cp","AS"]`]; than the low-risk control children r(`r myn["cp","LR"]`) = `r printr(myr["cp","LR"])`, CI [`r printr(mylb["cp","LR"])`,`r printr(myhb["cp","LR"])`], *p* `r printp(myp["cp","LR"])`.




## Additional analyses

In this section, we report on exploratory analyses, aimed at establishing conditions under which Zooniverse labels aligned with laboratory labels more or less closely. In previous work using a similar method, for instance, data from all three children from one dataset were often labeled as "Junk" (i.e., not a child's vocalization), and the data points from this corpus stood out when the authors attempted to integrate results with other corpora [@cychosz2021canonical]. A high proportion of "Junk" may indicate that automated segmentation was errorful for those children, and may be a sign that the rest of the data could be compromised as well. 

```{r prelim-Junk, echo=F}
JunkXdiag=t.test(proportions$z_Junk ~ proportions$Diagnosis)

#create matrix to hold r results
myr=matrix(NA,nrow=2,ncol=2)
rownames(myr)<-c("lp","cp")
colnames(myr)<-c("lower_Junk","higher_Junk")
mylb=myhb=myn=myp =myr #this is an ugly way of creating 5 matrices


  #get cor properties
  thiscor=cor.test(proportions[proportions$median_Junk=="lower_Junk","z_lp"],proportions[proportions$median_Junk=="lower_Junk","l_lp"])
    myr[1,"lower_Junk"]=round(thiscor$estimate,3)
    mylb[1,"lower_Junk"]=round(thiscor$conf.int[1],3)
    myhb[1,"lower_Junk"]=round(thiscor$conf.int[2],3)
    myn[1,"lower_Junk"]=round(thiscor$parameter,3)
    myp[1,"lower_Junk"]=round(thiscor$p.value,3)
    

  #repeat for LR  

      #get cor properties
  thiscor=cor.test(proportions[proportions$median_Junk!="lower_Junk","l_lp"],proportions[proportions$median_Junk!="lower_Junk","z_lp"])
    myr[1,"higher_Junk"]=round(thiscor$estimate,3)
    mylb[1,"higher_Junk"]=round(thiscor$conf.int[1],3)
    myhb[1,"higher_Junk"]=round(thiscor$conf.int[2],3)
    myn[1,"higher_Junk"]=round(thiscor$parameter,3)
    myp[1,"higher_Junk"]=round(thiscor$p.value,3)
    

    
    # repeat whole for cp
    

  #get cor properties
  thiscor=cor.test(proportions[proportions$median_Junk=="lower_Junk","z_cp"],proportions[proportions$median_Junk=="lower_Junk","l_cp"])
    myr[2,"lower_Junk"]=round(thiscor$estimate,3)
    mylb[2,"lower_Junk"]=round(thiscor$conf.int[1],3)
    myhb[2,"lower_Junk"]=round(thiscor$conf.int[2],3)
    myn[2,"lower_Junk"]=round(thiscor$parameter,3)
    myp[2,"lower_Junk"]=round(thiscor$p.value,3)
    

  #repeat for higher_Junk  

      #get cor properties
  thiscor=cor.test(proportions[proportions$median_Junk!="lower_Junk","l_cp"],proportions[proportions$median_Junk!="lower_Junk","z_cp"])
    myr[2,"higher_Junk"]=round(thiscor$estimate,3)
    mylb[2,"higher_Junk"]=round(thiscor$conf.int[1],3)
    myhb[2,"higher_Junk"]=round(thiscor$conf.int[2],3)
    myn[2,"higher_Junk"]=round(thiscor$parameter,3)
    myp[2,"higher_Junk"]=round(thiscor$p.value,3)
    
  
```


We investigated this hypothesis by calculating the proportion of data labeled as "Junk" for each individual child.  There was no significant difference in the proportions of their data labeled as "Junk" for children with Angelman syndrome (M = `r round(JunkXdiag$estimate["mean in group AngelmanSyndrome"]*100,2)`%, SD = `r round(sd(proportions[proportions$Diagnosis=="AngelmanSyndrome","z_Junk"])*100,2)`%) compared to the low-risk group (M = `r round(JunkXdiag$estimate["mean in group Low-RiskControl"]*100,2)`%, SD = `r round(sd(proportions[proportions$Diagnosis!="AngelmanSyndrome","z_Junk"])*100,2)`%): Welch's t(`r JunkXdiag$parameter`) = `r round(JunkXdiag$statistic,3)`, *p* = `r printp(JunkXdiag$p.value)`. We therefore collapsed across groups for this exploratory analysis, and split the 20 children using a median split on the proportion of their data labeled as "Junk". Results were similar across these two post-hoc subgroups (see Figure 6). For Linguistic Proportion, the correlation across lab and Zooniverse data for the lower Junk group was r(`r myn["lp","lower_Junk"]`) = `r printr(myr["lp","lower_Junk"])`, CI [`r printr(mylb["lp","lower_Junk"])`,`r printr(myhb["lp","lower_Junk"])`], *p* = `r printp(myp["lp","lower_Junk"])`]; and for the higher Junk group it was r(`r myn["lp","higher_Junk"]`) = `r printr(myr["lp","higher_Junk"])`, CI [`r printr(mylb["lp","higher_Junk"])`,`r printr(myhb["lp","higher_Junk"])`], *p* `r printp(myp["lp","higher_Junk"])`. For Canonical Proportion, the correlation across lab and Zooniverse data for the lower Junk group was r(`r myn["cp","lower_Junk"]`) = `r printr(myr["cp","lower_Junk"])`, CI [`r printr(mylb["cp","lower_Junk"])`,`r printr(myhb["cp","lower_Junk"])`], *p* `r printp(myp["cp","lower_Junk"])`]; and for the higher Junk group it was r(`r myn["cp","higher_Junk"]`) = `r printr(myr["cp","higher_Junk"])`, CI [`r printr(mylb["cp","higher_Junk"])`,`r printr(myhb["cp","higher_Junk"])`], *p* `r printp(myp["cp","higher_Junk"])`. Thus, it does not seem that a higher proportion of "Junk" labels is an index of low quality data.

----- INSERT FIGURE 6 HERE -----


Above, we concluded that derived metrics integrating information across audio clips (Linguistic and Canonical Proportion, which can be derived from segment- or chunk-level labels) seem more promising than segment-level data (where individual segments are classified into Crying, Laughing, etc.) Notice that our derived metrics do not require matching of chunks (500 ms presented to Zooniverse annotators) to segments (the original LENA&reg; segments presented to laboratory annotators), because one can calculate the proportions directly from the chunk-level judgment. As a result, there was one stage in our post-processing that may not have been necessary, whereby we collapsed labels across chunks associated to the same segment. We therefore repeated our analyses but instead of deriving our proportions for the Zooniverse data from the segment-level composite, we did it based on the individual chunk-level annotations. To facilitate comparison, correlations across laboratory- and Zooniverse-derived proportions are provided in Table 2.

----- INSERT TABLE 2 HERE -----


```{r bychunk-age, echo=F}
read.csv("zoo_chunk_majority_agreement.csv")->chunk
bychild=table(chunk$Answer,chunk$ChildID)
cans=bychild["Canonical",]
noncans=bychild["Non-Canonical",]
Junk=bychild["Junk",]
sum_bychild=table(chunk$ChildID)

ling_prop =(cans+noncans)/(sum_bychild-Junk)
can_prop =cans/(cans+noncans)

#check that they match before merging
#demo_data$ChildID==names(ling_prop)
#demo_data$ChildID==names(can_prop)

props_bychild_chunk=cbind(ling_prop,can_prop,demo_data)
colnames(props_bychild_chunk)[1:2]<-c("chi","ling_prop")

z_lp_as=cor.test(props_bychild_chunk$ling_prop[props_bychild_chunk$Diagnosis=="AngelmanSyndrome"],props_bychild_chunk$Age[props_bychild_chunk$Diagnosis=="AngelmanSyndrome"])

z_lp_lr=cor.test(props_bychild_chunk$ling_prop[props_bychild_chunk$Diagnosis!="AngelmanSyndrome"],props_bychild_chunk$Age[props_bychild_chunk$Diagnosis!="AngelmanSyndrome"])

z_cp_as=cor.test(props_bychild_chunk$can_prop[props_bychild_chunk$Diagnosis=="AngelmanSyndrome"],props_bychild_chunk$Age[props_bychild_chunk$Diagnosis=="AngelmanSyndrome"])

z_cp_lr=cor.test(props_bychild_chunk$can_prop[props_bychild_chunk$Diagnosis!="AngelmanSyndrome"],props_bychild_chunk$Age[props_bychild_chunk$Diagnosis!="AngelmanSyndrome"])

#matrix that will hold all r results
corage=cbind(corage[,c("l_lp","z_lp")],
             t(cbind(printr(z_lp_as$estimate),printr(z_lp_lr$estimate))),
             corage[,c("l_cp","z_cp")],
             t(cbind(printr(z_cp_as$estimate),printr(z_cp_lr$estimate)))
             )
corage=t(corage)
rownames(corage)<-c("LP (lab)","LP (Zoon. Seg.)","LP (Zoon. Chunks)",
                    "CP (lab)","CP (Zoon. Seg.)","CP (Zoon. Chunks)")
```


Regarding correlations with age, we found very similar results to the analyses based on laboratory annotations, as well as those based on Zooniverse annotations collapsed at the segment level (see Table 3). That is, the Linguistic Proportion was not strongly associated with age in the Angelman syndrome group: r(`r z_lp_as$parameter`) = `r printr(z_lp_as$estimate)`, CI [`r printr(z_lp_as$conf.int[1])`,`r printr(z_lp_as$conf.int[2])`], *p* = `r printp(z_lp_as$p.value)`], but increased with age  in the low-risk control group: r(`r z_lp_lr$parameter`) = `r printr(z_lp_lr$estimate)`, CI [`r printr(z_lp_lr$conf.int[1])`,`r printr(z_lp_lr$conf.int[2])`], *p* = `r printp(z_lp_lr$p.value)`. Likewise, the Canonical Proportion did not exhibit the same pattern across the groups, with a non-significant developmental decrease found among children with Angelman syndrome r(`r z_cp_as$parameter`) = `r printr(z_cp_as$estimate)`, CI [`r printr(z_cp_as$conf.int[1])`,`r printr(z_cp_as$conf.int[2])`], *p* = `r printp(z_cp_as$p.value)`]; and a marginal developmental increase among low-risk control r(`r z_cp_lr$parameter`) = `r printr(z_cp_lr$estimate)`, CI [`r printr(z_cp_lr$conf.int[1])`,`r printr(z_cp_lr$conf.int[2])`], *p* = `r printp(z_cp_lr$p.value)`.

----- INSERT TABLE 3 HERE -----


```{r zoo-lab-chunk, echo=F}

#create matrix to hold r results
myr=matrix(NA,nrow=2,ncol=3)
rownames(myr)<-c("lp","cp")
colnames(myr)<-c("AS","LR","all")
mylb=myhb=myn=myp =myr #this is an ugly way of creating 5 matrices

proportions=proportions[order(proportions$ChildID),]

  #get cor properties
  thiscor=cor.test(props_bychild_chunk[props_bychild_chunk$Diagnosis=="AngelmanSyndrome","ling_prop"],proportions[proportions$Diagnosis=="AngelmanSyndrome","l_lp"])
    myr[1,"AS"]=round(thiscor$estimate,3)
    mylb[1,"AS"]=round(thiscor$conf.int[1],3)
    myhb[1,"AS"]=round(thiscor$conf.int[2],3)
    myn[1,"AS"]=round(thiscor$parameter,3)
    myp[1,"AS"]=round(thiscor$p.value,3)
    

  #repeat for LR  


      #get cor properties
  thiscor=cor.test(props_bychild_chunk[props_bychild_chunk$Diagnosis!="AngelmanSyndrome","ling_prop"],proportions[proportions$Diagnosis!="AngelmanSyndrome","l_lp"])
    myr[1,"LR"]=round(thiscor$estimate,3)
    mylb[1,"LR"]=round(thiscor$conf.int[1],3)
    myhb[1,"LR"]=round(thiscor$conf.int[2],3)
    myn[1,"LR"]=round(thiscor$parameter,3)
    myp[1,"LR"]=round(thiscor$p.value,3)
    
#repeat for all cor
    
      thiscor=cor.test(props_bychild_chunk$ling_prop,proportions$l_lp)
    myr[1,"all"]=round(thiscor$estimate,3)
    mylb[1,"all"]=round(thiscor$conf.int[1],3)
    myhb[1,"all"]=round(thiscor$conf.int[2],3)
    myn[1,"all"]=round(thiscor$parameter,3)
    myp[1,"all"]=round(thiscor$p.value,3)
    
    # repeat whole for cp


  #get cor properties
  thiscor=cor.test(props_bychild_chunk[props_bychild_chunk$Diagnosis=="AngelmanSyndrome","can_prop"],proportions[proportions$Diagnosis=="AngelmanSyndrome","l_cp"])
    myr[2,"AS"]=round(thiscor$estimate,3)
    mylb[2,"AS"]=round(thiscor$conf.int[1],3)
    myhb[2,"AS"]=round(thiscor$conf.int[2],3)
    myn[2,"AS"]=round(thiscor$parameter,3)
    myp[2,"AS"]=round(thiscor$p.value,3)
    

  #repeat for LR  


      #get cor properties
  thiscor=cor.test(props_bychild_chunk[props_bychild_chunk$Diagnosis!="AngelmanSyndrome","can_prop"],proportions[proportions$Diagnosis!="AngelmanSyndrome","l_cp"])
    myr[2,"LR"]=round(thiscor$estimate,3)
    mylb[2,"LR"]=round(thiscor$conf.int[1],3)
    myhb[2,"LR"]=round(thiscor$conf.int[2],3)
    myn[2,"LR"]=round(thiscor$parameter,3)
    myp[2,"LR"]=round(thiscor$p.value,3)
    
#repeat for all cor
    
      thiscor=cor.test(props_bychild_chunk$can_prop,proportions$l_cp)
    myr[2,"all"]=round(thiscor$estimate,3)
    mylb[2,"all"]=round(thiscor$conf.int[1],3)
    myhb[2,"all"]=round(thiscor$conf.int[2],3)
    myn[2,"all"]=round(thiscor$parameter,3)
    myp[2,"all"]=round(thiscor$p.value,3)
    
 mycortab=rbind(mycortab,t(myr))
rownames(mycortab)[4:6]<-paste("Chunks",rownames(t(myr)))

```


As for correlations between Zooniverse- and laboratory-derived metrics (e.g., the correlation between the Linguistic Proportion as estimated using Zooniverse coding at the chunk level and the Linguistic Proportion via laboratory coding at the segment level), we observe very similar levels of correlation to the segment-based analyses (see Table 2): Linguistic Proportion overall r(`r myn["lp","all"]`) = `r printr(myr["lp","all"])`, CI [`r printr(mylb["lp","all"])`,`r printr(myhb["lp","all"])`], *p* `r printp(myp["lp","all"])`] between Linguistic Proportion by child from the Zooniverse at the chunk level and the lab annotators' data at the segment level.  When we split by participant group, correlations remain high: for Angelman syndrome r(`r myn["lp","AS"]`) = `r printr(myr["lp","AS"])`, CI [`r printr(mylb["lp","AS"])`,`r printr(myhb["lp","AS"])`], *p* = `r printp(myp["lp","AS"])`; low-risk control r(`r myn["lp","LR"]`) = `r printr(myr["lp","LR"])`, CI [`r printr(mylb["lp","LR"])`,`r printr(myhb["lp","LR"])`], *p* = `r printp(myp["lp","LR"])`. As for Canonical Proportion overall r(`r myn["cp","all"]`) = `r printr(myr["cp","all"])`, CI [`r printr(mylb["cp","all"])`,`r printr(myhb["cp","all"])`], *p* `r printp(myp["cp","all"])`; Angelman syndrome: r(`r myn["cp","AS"]`) = `r printr(myr["cp","AS"])`, CI [`r printr(mylb["cp","AS"])`,`r printr(myhb["cp","AS"])`], *p* = `r printp(myp["cp","AS"])`;  low-risk control infants r(`r myn["cp","LR"]`) = `r printr(myr["cp","LR"])`, CI [`r printr(mylb["cp","LR"])`,`r printr(myhb["cp","LR"])`], *p* `r printp(myp["cp","LR"])`.


Since all of these results are very similar to those obtained when collapsing across chunk-level labels to generate segment-level labels, we conclude that in the future this step can be skipped. Instead, researchers can derive Linguistic and Canonical Proportions directly from citizen scientists' chunk-level labels [which was in fact what @cychosz2021canonical did].


```{r downsample}

#we start from the data base that has all the info
nsegs=data.frame(table(data_all$ChildID))
colnames(nsegs)<-c("ChildID","nsegs")

merge(nsegs,demo_data)->nsegs
nseg_t=t.test(nsegs$nsegs~nsegs$Diagnosis)

resampling=matrix(NA,nrow=50,ncol=6)
colnames(resampling)<-c("lp_r_all","lp_r_as","lp_r_lr","cp_r_all","cp_r_as","cp_r_lr")

for(i in 1:50){
  sel_seg=NULL
  
  #the next line samples 100 voc IDs from each child
  for(thischi in levels(factor(data_all$ChildID))) sel_seg<-c(sel_seg,as.character(sample(data_all$segmentId_DB[data_all$ChildID==thischi],100)))
  
  #generate a subset using those voc IDs
  sampling=data_all[as.character(data_all$segmentId_DB) %in% as.character(sel_seg),]
  
  #get the ns by child, then calculate the linguistic proportion & canonical proportion, separately for zooniverse & lab
  ztab=table(sampling$ChildID,sampling$Zoon_classif)
  z_lp =rowSums(ztab[,c("Canonical","Non-Canonical")])/rowSums(ztab[,-which(colnames(ztab) %in% c("Junk"))])
  z_cp =ztab[,c("Canonical")]/rowSums(ztab[,c("Canonical","Non-Canonical")])

  ltab=table(sampling$ChildID,sampling$lab)
  l_lp =rowSums(ltab[,c("Canonical","Non-Canonical")])/rowSums(ltab[,-which(colnames(ztab) %in% c("Junk"))])
  l_cp =ltab[,c("Canonical")]/rowSums(ltab[,c("Canonical","Non-Canonical")])
  
  thistab=cbind(demo_data,z_lp,z_cp,l_lp,l_cp)
  
  resampling[i,"lp_r_all"]=cor.test(thistab$z_lp,thistab$l_lp)$estimate
  resampling[i,"lp_r_as"]=cor.test(thistab$z_lp[thistab$Diagnosis=="AngelmanSyndrome"],thistab$l_lp[thistab$Diagnosis=="AngelmanSyndrome"])$estimate
  resampling[i,"lp_r_lr"]=cor.test(thistab$z_lp[thistab$Diagnosis!="AngelmanSyndrome"],thistab$l_lp[thistab$Diagnosis!="AngelmanSyndrome"])$estimate

   resampling[i,"cp_r_all"]=cor.test(thistab$z_cp,thistab$l_cp)$estimate
  resampling[i,"cp_r_as"]=cor.test(thistab$z_cp[thistab$Diagnosis=="AngelmanSyndrome"],thistab$l_cp[thistab$Diagnosis=="AngelmanSyndrome"])$estimate
  resampling[i,"cp_r_lr"]=cor.test(thistab$z_cp[thistab$Diagnosis!="AngelmanSyndrome"],thistab$l_cp[thistab$Diagnosis!="AngelmanSyndrome"])$estimate
}


myr=colMeans(resampling)
mycortab=rbind(mycortab,cbind(myr[c("lp_r_as" ,"lp_r_lr","lp_r_all")],myr[c("cp_r_as" ,"cp_r_lr","cp_r_all")]))
rownames(mycortab)[7:9]<-c("100 seg AS","100 seg LR","100 seg all")
for(i in 1:dim(mycortab)[1]) for(j in 1:dim(mycortab)[2]) mycortab[i,j]<-printr(as.numeric(as.character(mycortab[i,j])))


resampling=data.frame(resampling)
```

Next, we looked at whether having more segments from each child may lead to more reliable metrics.  We observed a non-significant trend [Welch t(`r nseg_t$parameter`)=`r nseg_t$statistic`] for lower number of segments in the Angelman syndrome group (M = `r nseg_t$estimate[1]`, range `r min(nsegs$nsegs[nsegs$Diagnosis=="AngelmanSyndrome"])`-`r max(nsegs$nsegs[nsegs$Diagnosis=="AngelmanSyndrome"])`) than among low-risk control infants (M = `r nseg_t$estimate[2]`, range `r min(nsegs$nsegs[nsegs$Diagnosis!="AngelmanSyndrome"])`-`r max(nsegs$nsegs[nsegs$Diagnosis!="AngelmanSyndrome"])`), likely because of the lower volubility among the former group of children. Moreover, these numbers of segments are much larger than those used in previous work relying on citizen science classifications [@cychosz2021canonical]. We therefore asked whether the correlations between laboratory and Zooniverse child-level estimates are affected by how much data were extracted from each child by under-sampling. Since @cychosz2021canonical extracted 100 segments from each child in their study, we randomly sampled 100 segments from each of our children's data, and recalculated the Linguistic and Canonical Proportions based only on these 100 labels. We repeated this process 50 times, to assess the extent to which the association between laboratory- and Zooniverse-derived metrics varied in each random sample (e.g., the correlation between Linguistic Proportion as estimated using Zooniverse coding on the one hand, and Linguistic Proportion as estimated using laboratory coding on the other). 

The mean correlations across 50 runs were lower than those recovered using all segments from each child (see Table 2, "100 seg" rows), particularly for the low-risk younger infants, and for Linguistic Proportion. This may indicate that, especially in some groups of infants, 100 segments may not be sufficient to capture a stable estimate of the child's Linguistic and Canonical Proportions. We also observed quite a bit of variance across the 50 runs. For Linguistic Proportion, the range of correlations found for all infants was [`r printr(min(resampling$lp_r_all))`,`r printr(max(resampling$lp_r_all))`]; for the Angelman syndrome group [`r printr(min(resampling$lp_r_as))`,`r printr(max(resampling$lp_r_as))`]; for the low-risk group [`r printr(min(resampling$lp_r_lr))`,`r printr(max(resampling$lp_r_lr))`].  For Canonical Proportion, the range of correlations found for all infants was [`r printr(min(resampling$cp_r_all))`,`r printr(max(resampling$cp_r_all))`]; for the Angelman syndrome group [`r printr(min(resampling$cp_r_as))`,`r printr(max(resampling$cp_r_as))`]; for the low-risk group [`r printr(min(resampling$cp_r_lr))`,`r printr(max(resampling$cp_r_lr))`].





## Discussion

In the present study we assessed the extent to which child vocalizations from LENA&reg; daylong recordings can be accurately described based on classifications collected using the citizen science platform Zooniverse. Our recordings came from children diagnosed with a genetic syndrome associated with severe language disorder, as well as low-risk controls, and included a range of different ages. Excerpts of these recordings had previously been annotated by trained experts in the lab, who provided the gold standard.

In descriptive terms, we found that category frequency was similar across laboratory and citizen science annotations. Correlations between age and our two child-level metrics revealed different developmental patterns between our sample of older children with a diagnosis of Angelman syndrome and younger low-risk controls. Given that the two groups of children differ on both of these features (age and diagnosis), we cannot empirically tease apart these two differences. Nonetheless, inspection of Figure 3 suggests different interpretations for the two derived metrics. Overall, the pattern for Linguistic Proportion is compatible with a non-linear trajectory common to the two groups, whereby the Linguistic Proportion increases rapidly up to 20 months, and hovers around 90-100% thereafter. In this case, the lack of correlation with age in the Angelman syndrome group suggests that there is little systematic variance in this metric after 20 months of age. In contrast, the two groups seem to differ in their trajectory for Canonical Proportion, with rapid increases up to 20 months in the low-risk group, and relatively stable levels of Canonical Proportion among the children in the Angelman syndrome group [in line with previous work; @hamrick2019capturing;@hamrick2019measurement]. 

Next, we looked at agreement between laboratory and Zooniverse annotation at the level of individual segments. We found that some categories had high precision and recall -- notably Non-canonical segments were detected quite accurately, and to a lesser extent the same was true for Canonical segments. This is interesting because it suggests that Zooniverse annotations may be trusted particularly for detecting these two types of speech-like segments.

In other cases, precision was fairly high but recall was lower. This was the case of the "Junk/Don't mark" category. The combination of high precision and low recall indicates that when Zooniverse annotations return a "Junk" judgment, this tends to be correct, but Zooniverse annotations may not detect all elements labeled as "Don't mark" in the laboratory. Inspection of confusion matrices and overall frequencies suggest that such a pattern of errors could be due to citizen scientists accepting more clips (i.e., using the "Junk" category less) than laboratory annotators, who have access to the whole segment (and its context). This is consistent with the fact that our lab definition of "Junk" was more stringent than the Zooniverse definition. Indeed, laboratory coders were instructed to be sensitive since the aim was also to study acoustic characteristics of the segments (which is unrelated to the current project), and therefore they would use "Don't mark" if there was *any* noise or overlapping speech that could affect acoustic analyses.

Both Crying and Laughing showed moderate to good recall, but low precision. This means that a high proportion of segments detected as these two categories in the lab can be classified in the appropriate category by Zooniverse annotators, but Zooniverse annotators also put into these categories many segments that lab annotators classified in a different manner. Most saliently, there is a problematic level of confusion between Crying and Non-Canonical, whereby lab-detected Crying was almost as likely to be classified as such or Non-canonical in Zooniverse (the recall was `r round(stall_r[stall_r$Lab=="Crying" & stall_r$Zooniverse=="Crying","rec"])` and `r round(stall_r[stall_r$Lab=="Crying" & stall_r$Zooniverse=="Non-Canonical","rec"])`% respectively), and even worse, 55% of the segments Zooniverse annotators classified as Crying were actually Non-Canonical according to lab annotations. This is probably due to the fact that lab annotators had access to the whole segment, and could even listen to the context of that segment, so they could accurately interpret a segment as crying. Thus, it is possible that additional analyses, using for instance pitch or duration characteristics at the level of the whole segment rather than the chunk could improve classification performance, but for the time being, we cannot recommend Zooniverse annotations to identify crying bouts. 

As for Laughing, the recall is quite good but precision is low, with confusion involving "Junk" and Non-Canonical to similar extents. We are tentative in our interpretation of this result because the category was very sparsely represented, with barely `r table(data_all$lab)["Laughing"]` segments classified as Laughing in the whole data set. We thus recommend additional work, although we expect that perhaps there will be a similar problem (and solution) as indicated for Crying, whereby determination of Laughing versus Non-Canonical will require inspection of segment-level properties.

Turning now to the child-level analyses, we were particularly interested in derived metrics because this is more typically the goal of such analyses, for instance to study potential individual or group differences, or to investigate the impact of an intervention on a child or group of children. Our results show that, despite only moderate levels of agreement at the level of the segment, our two derived metrics (Linguistic and Canonical Proportions) were highly correlated across laboratory and Zooniverse data, meaning that variability in precision and recall did not substantially impact key outcome variables.  Errors only seemed systematic for the Linguistic Proportion (and not for Canonical Proportion), and they were due to lower Linguistic Proportions from Zooniverse than laboratory data. This is due to the fact that a substantial number of Crying segments were classified as Non-Canonical by Zooniverse annotators. Nonetheless, since these errors were systematic across infants, they do not affect the study of group or individual variation.

Our exploratory analyses then dug into some methodological aspects of our data. We found that, although individuals vary widely in terms of what proportion of their data is classified as Junk by Zooniverse annotators, this is not a sign that their data as a whole is compromised, with similar levels of correlations between laboratory- and Zooniverse-derived metrics across median groups based on proportion of their data classified as Junk. 

Next, we found that more data led to more reliable measurements: Specifically, our correlations based on `r min(nsegs$nsegs)`-`r max(nsegs$nsegs)` segments per child were markedly higher than those based on 100 segments per child, particularly when looking at our low-risk, younger participants. It is important to note that an advantage of Zooniverse, compared to laboratory annotations, is that it is easier to scale up: More data from each individual child can be annotated this way, which helps countering noise. 


In sum, we believe these data show that young children's vocalizations can be accurately described by relying on the judgments of Zooniverse's citizen scientists, particularly when the goal is to describe vocalizations at a broader level than individual chunks or segments. It is important to note that from a methodological point of view, the use of crowdsourcing is not merely a question of being an ecological technique, but it can also offer benefits in the area of scientific rigor and reproducibility. As previously mentioned in the Introduction section, Zooniverse offers both an automated system of data handling as well as an enormous pool of users: access to a large-scale group of blinded annotators can in itself help researchers overcome the methodological compromises that sometimes lead to biased results. Additionally, crowd-sourcing and open-science frameworks can come together to encourage replication of existing work: open-source software, shared data and citizen science platforms would make it possible, and actually easy, to run identical studies and evaluate research results independently.

### Further research directions

These findings open up a new, exciting avenue for future research. Our pipeline to upload data on Zooniverse is scalable and quickly adapted to new datasets, including data on infants of different ages, learning different languages, or living in environments with different acoustic properties (e.g., rural versus urban environments). This means large existing datasets of daylong recordings, notably those hosted on Homebank [@vandam2016homebank], can be reliably annotated in an ecological way using citizen science. 

Notwithstanding our enthusiasm, we would like to point out some limitations of our results that should be addressed in future research. The first area in which more work would be welcome involves the pre-processing. We used LENA&reg; algorithms to detect vocalizations by the key child, but there are newer, better-performing, open-sourced alternatives that could be used instead [@lavechin2020open]. Still, performance is most often established on children whose development is thought to be normative, being raised in small households located in urban environments, whereas it would be important to check for accuracy in more diverse samples, where the children have a diagnosis [e.g., @Dykstra2013], multiple siblings [e.g., @Elo], and/or are growing up in a rural environment [e.g., @cristia2020thorough]. Speech technology developments would also be welcome to complement citizen scientists' classifications, particularly for categories that may be hard to classify based on short samples. There is a great deal of work to be done in this sense. The Introduction mentioned a challenge that established a baseline performance with unweighted average recall (UAR) of 54% (when recall is averaged across the 5 classes, Laughing, Crying, Canonical, Non-canonical, and Junk, giving equal weight to each of them rather than weighing them based on their frequency of occurrence). As reported in Table 1, our own method leads to a UAR of `r round(mean(stall_r$rec[stall_r$Lab==stall_r$Zooniverse]))`%, so one may think there is not much to gain with the automated method. However, notice that the automated method does lead to much better performance than Zooniverse classifications for Crying, and to some improvements in Canonical and Junk. For instance, the team who won the challenge in 2019 improved UAR by about 2%, primarily through gains in the laughing class obtained by adding training data [@yeh2019using]. That state of the art was challenged by @kaya2020combining, who obtained a UAR of 61% on the same data as the challenge, thanks to improvements in all of the classes but for Laughing. Even more recently, @yao2020classification reported on a series of classifiers focused on crying, with astounding improvements in this class. Thus, there may be some gains to be obtained in specific vocalization types, particularly those that are rare (like Laughing) and those which require context (Crying). 

Our results warrant additional research on two metrics we used to describe children’s linguistic productions: Linguistic Proportion and Canonical Proportion. Linguistic Proportion has not been investigated extensively in previous research, and correlations with age were weaker than those for Canonical Proportion. We hope additional research is devoted to further investigating this measure as a child-level descriptor, since it may be more relevant to emotional disorders [see a review in @halpern2016excessive]. Additionally, while metrics conceptually related to our Canonical Proportion have been studied in much previous work, we believe further research is necessary using larger scales (saliently increasing the sheer number of children studied, to have sufficient power to detect group effects) and with broader coverage [notably including children growing up in a broad range of languages and cultures, as in @cychosz2021canonical], to make sure that assumptions regarding cross-linguistic and maturational universality are amply justified and not merely a result of having low power to detect differences, or simply sampling from similar populations.

Assuming these child-level metrics continue to be supported by such additional data, it would be interesting for future investigations to expand the analysis of Linguistic Proportion and Canonical Proportion to recover differences and similarities in language development between low-risk children and children with disorders other than Angelman syndrome. That is, it is at present unclear whether relatively lower Canonical Proportions are also be found in other populations, or whether this is more specific to Angelman syndrome. For instance, some previous work has found differences in babbling even when comparing late talkers with toddlers with no specific diagnosis  [@fasolo2008babbling]. We thus encourage further research inspecting Canonical Proportion in other populations, as well as methodological work that attempts to generate metrics that are similarly easy to gather at scale. 


We also acknowledge that work on children with non-normative development poses in itself some challenges. Our data contained a lower number of vocalizations for children diagnosed with Angelman syndrome, even though we selected much younger low-risk control children as the comparison group. However, this limitation could be easily overcome in the future: We were limited here because we wanted to use the exact same segments that laboratory annotators had inspected. Now that this method has been validated, researchers can extract all segments that LENA&reg; identifies as being the key child, thus maximally using available data to get more reliable estimates, as suggested by one of our exploratory analyses. This approach may be particularly useful in the case of data from children with a diagnosis, whose data may be under-represented in LENA&reg; research, compared to data from normative children. We also hope future work makes use of the scalability of the method by employing larger sample sizes than those used in the present study, where data were drawn from only 10 children in each group. 



Another open question we hope future work looks into is the assessment of within-participant changes. Our dataset only contained one daylong recording per child (with one exception), so other datasets in which a greater number of recordings are collected longitudinally can bring novel insights. Is the age-related decline we observed among children in the Angelman syndrome group truly a developmental decline that happens by necessity in this population, or might it reflect partially other aspects of children's experience and behavior?  Additionally, longitudinal work within the same infant may be needed to track the "natural history" of vocalizations to better contextualize how these metrics should be interpreted. It is also an open question how large  effects must be, at the individual level, in order to be detected with this method. For example, there is interest in using naturalistic recordings as outcome measures for clinical (behavioral or pharmacological) trials. The fact that agreement across annotators is high even in the data coming from children diagnosed with Angelman syndrome is encouraging, but it would be important to establish whether absolute levels of Canonical Proportion, and/or rates of change over longer time spans, may be good candidates for tracking the effects of a given treatment. 


Finally, broader generalizations of our technique could be of interest to readers of this paper. Any researcher is able to undergo the procedure to create Zooniverse projects (https://www.zooniverse.org/about/faq). For instance, a project could be set up to annotate potentially disordered speech by older adults at risk of, or diagnosed with, neurodegenerative diseases. Although it would be important to similarly validate such annotations, we believe there is strong promise for such tasks, because many neurodegenerative disorders affect local aspects of the speech, which are detected at the syllable level. In contrast, tasks that require more speech or audio context may not be well suited to citizen science platforms, because they would require playing longer clips, which may reveal identifying or sensitive information. Although citizen scientists themselves are well-intentioned, the platform could be used by others who want to exploit the system for nefarious goals.


## Conclusion

In this study, we validated the quality of annotations obtained through a citizen science platform, Zooniverse, as compared to a gold standard of human expert annotators. We analyzed the correspondence between annotations at the individual segment level, and the individual child level, using Canonical Proportion and Linguistic Proportion as descriptors. We found moderate to good accuracy in the former, and strong positive correlations in the latter. We can conclude that citizen scientists are a reliable source of fast and ecological annotation of speech data, particularly when results are combined into child-level descriptors. The same methodology may be applied to several research questions in the study of language acquisition and language disorders. This finding is particularly welcome in an era when wearables open new avenues for studying human behavior and development in an ecological manner.


\newpage

# Acknowledgements 

We are grateful to the families who contributed their data; to the Zooniverse volunteers that made this work possible; and the audience and technical committee of SLT 2020 for helpful feedback on preliminary analyses.





``` {r fig-zoo, comment=F, message=F, hide = T, warning = F, echo = F,  fig.cap ="Screen captures of the 6 stages of the tutorial, three of which are shown here: The initial explanation of the trial, examples of canonical vocalizations, and the classification choice the user needed to make; as well as the Field Guide, which remained accessible on the right of the screen throughout the session, and which allowed users to revise examples of the different categories."}
knitr::include_graphics("zooniverse-pufig.pdf")
```



```{r fig-process, comment=F, message=F, hide = T, warning = F, echo = F, fig.cap = "Correspondence between LENA-identified segments (which can contain one or more vocalizations) and chunks. Segments and chunks are collectively referred to as clips because they are parts of the longer audio recording. Notice that clips are first annotated by multiple people, and these judgments are then combined into a single label based on simple majority. To compare laboratory and Zooniverse annotations at the level of segments, chunk-level labels are combined using the hierarchy Canonical > Non-canonical > Crying > Laughing > Junk."}
knitr::include_graphics("fig_levels.key.pdf")

```


```{r fig-corage, echo=F,fig.cap ="Correlations between child-level descriptors and age as a function of metric (Linguistic Proportion in the top row, Canonical Proportion in the bottom row), annotation method, and child group (red circles = low-risk, and black crosses = Angelman syndrome)."}

prettynames=c("Linguistic Proportion (Zoon.)","Canonical Proportion (Zoon.)",
             "Linguistic Proportion (Lab)","Canonical Proportion (Lab)" )
names(prettynames)<-c("z_lp","z_cp","l_lp","l_cp")
mycols=c("black","red")
names(mycols)<-c("Low-RiskControl","AngelmanSyndrome")
mypch=c(4,20)
names(mypch)<-c("Low-RiskControl","AngelmanSyndrome")

lp_range=range(proportions[,c("z_lp","l_lp")])
cp_range=range(proportions[,c("z_cp","l_cp")])

#jpeg("../Results/corage.jpg",width=20,height=20,units="cm",res=300)
layout(matrix(c(1:4), 2, 2, byrow = F))
par(mar = c(4, 4, 1, 1))

#create matrix to hold r results
myr=matrix(NA,nrow=4,ncol=2)
rownames(myr)<-c("z_lp","z_cp","l_lp","l_cp")
colnames(myr)<-c("AS","LR")
mylb=myhb=myn=myp =myr #this is an ugly way of creating 5 matrices

for(thisvar in c("z_lp","z_cp","l_lp","l_cp")) {

  if(thisvar %in% c("z_lp","l_lp")) myrange=lp_range else myrange=cp_range
    
  plot(proportions[,thisvar]~proportions$Age, pch=mypch[proportions$Diagnosis],xlab="Age (months)",ylab=prettynames[thisvar],
       col=mycols[proportions$Diagnosis],
         ylim=myrange)
  abline(lm(proportions[,thisvar]~proportions$Age,subset=c(proportions$Diagnosis=="AngelmanSyndrome")),col="black")

  #get cor properties
  thiscor=cor.test(proportions[proportions$Diagnosis=="AngelmanSyndrome",thisvar],proportions$Age[proportions$Diagnosis=="AngelmanSyndrome"])
    myr[thisvar,"AS"]=round(thiscor$estimate,3)
    mylb[thisvar,"AS"]=round(thiscor$conf.int[1],3)
    myhb[thisvar,"AS"]=round(thiscor$conf.int[2],3)
    myn[thisvar,"AS"]=round(thiscor$parameter,3)
    myp[thisvar,"AS"]=round(thiscor$p.value,3)
    
    #add r to figure
  text(mean(proportions$Age[proportions$Diagnosis=="AngelmanSyndrome"]),mean(proportions[proportions$Diagnosis=="AngelmanSyndrome",thisvar]),paste0("r=",myr[thisvar,"AS"]),col="black")

  #repeat for LR  
    abline(lm(proportions[,thisvar]~proportions$Age,subset=c(proportions$Diagnosis!="AngelmanSyndrome")),col="red")

      #get cor properties
  thiscor=cor.test(proportions[proportions$Diagnosis!="AngelmanSyndrome",thisvar],proportions$Age[proportions$Diagnosis!="AngelmanSyndrome"])
    myr[thisvar,"LR"]=round(thiscor$estimate,3)
    mylb[thisvar,"LR"]=round(thiscor$conf.int[1],3)
    myhb[thisvar,"LR"]=round(thiscor$conf.int[2],3)
    myn[thisvar,"LR"]=round(thiscor$parameter,3)
    myp[thisvar,"LR"]=round(thiscor$p.value,3)
    
    #add r to figure
    
    text(mean(proportions$Age[proportions$Diagnosis!="AngelmanSyndrome"]),mean(proportions[proportions$Diagnosis!="AngelmanSyndrome",thisvar]),paste0("r=",myr[thisvar,"LR"]),col="red")
}
#dev.off()
```



```{r fig-prec-rec, echo=F,fig.cap ="Number of segments as a function of their majority label from laboratory annotations (rows) versus Zooniverse annotations (columns). Percentages (and shading) indicate precision (left) and recall (right). For precision, the percentage indicates what percentage of the segments that have a given majority label according to Zooniverse data are attributed a given label in Lab data (i.e., columns add up to 100%). For recall, the percentage indicates what percentage of the segments that have a given majority label according to Lab data are attributed a given label in Zooniverse data (i.e., rows add up to 100%)."}

stall_p$Lab=factor(stall_p$Lab,levels=label_options)
stall_p$Zooniverse=factor(stall_p$Zooniverse,levels=label_options)

prec_plot = ggplot(data = stall_p, mapping = aes(y = Lab, x=Zooniverse)) +
 geom_tile(aes(fill= rescale(pr)), colour = "white") +
  geom_text(aes(label = paste(round(pr),"%")), vjust = -1,size=4) +
  geom_text(aes(label = Freq), vjust = 1,size=4) +
  scale_fill_gradient(low = "white", high = "red", name = "Percentage") +
     theme(legend.position = "none") +
  xlab("Zooniverse") + ylab("Lab") +
  ggtitle("Precision")+ theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

rec_plot = ggplot(data = stall_r, mapping = aes(y = Lab, x=Zooniverse)) +
 geom_tile(aes(fill= rescale(rec)), colour = "white") +
  geom_text(aes(label = paste(round(rec),"%")), vjust = -1,size=4) +
  geom_text(aes(label = Freq), vjust = 1,size=4) +
  scale_fill_gradient(low = "white", high = "red", name = "Percentage") +
     theme(legend.position = "none") +
  xlab("Zooniverse") + ylab("Lab") +
  ggtitle("Recall")+ theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

ggarrange(prec_plot, rec_plot, 
          labels = c("A", "B"),
          ncol = 2, nrow = 1)

```



```{r fig-corlab-zoo, echo=F,fig.cap ="Correlations between child-level descriptors derived from Zooniverse data (y axis) versus lab data (x axis) as a function of child group (red circles = low-risk, and black crosses = Angelman syndrome)."}

layout(matrix(c(1:2), 1, 2, byrow = F))
par(mar = c(4, 4, 1, 1))

#create matrix to hold r results
myr=matrix(NA,nrow=2,ncol=3)
rownames(myr)<-c("lp","cp")
colnames(myr)<-c("AS","LR","all")
mylb=myhb=myn=myp =myr #this is an ugly way of creating 5 matrices

lims=range(c(proportions[,"z_lp"],proportions[,"l_lp"]))

  plot(proportions[,"z_lp"]~proportions[,"l_lp"], xlab=prettynames["l_lp"],ylab=prettynames["z_lp"],
       xlim=lims,ylim=lims,
       col=mycols[proportions$Diagnosis], pch=mypch[proportions$Diagnosis])
  lines(c(0,1),c(0,1),lty=2,col="darkgray")

  abline(lm(proportions[,"z_lp"]~proportions[,"l_lp"],subset=c(proportions$Diagnosis=="AngelmanSyndrome")),col="black")

  #get cor properties
  thiscor=cor.test(proportions[proportions$Diagnosis=="AngelmanSyndrome","z_lp"],proportions[proportions$Diagnosis=="AngelmanSyndrome","l_lp"])
    myr[1,"AS"]=round(thiscor$estimate,3)
    mylb[1,"AS"]=round(thiscor$conf.int[1],3)
    myhb[1,"AS"]=round(thiscor$conf.int[2],3)
    myn[1,"AS"]=round(thiscor$parameter,3)
    myp[1,"AS"]=round(thiscor$p.value,3)
    

  #repeat for LR  
    abline(lm(proportions[,"z_lp"]~proportions[,"l_lp"],subset=c(proportions$Diagnosis!="AngelmanSyndrome")),col="red")

      #get cor properties
  thiscor=cor.test(proportions[proportions$Diagnosis!="AngelmanSyndrome","l_lp"],proportions[proportions$Diagnosis!="AngelmanSyndrome","z_lp"])
    myr[1,"LR"]=round(thiscor$estimate,3)
    mylb[1,"LR"]=round(thiscor$conf.int[1],3)
    myhb[1,"LR"]=round(thiscor$conf.int[2],3)
    myn[1,"LR"]=round(thiscor$parameter,3)
    myp[1,"LR"]=round(thiscor$p.value,3)
    
#repeat for all cor
    
      thiscor=cor.test(proportions$z_lp,proportions$l_lp)
    myr[1,"all"]=round(thiscor$estimate,3)
    mylb[1,"all"]=round(thiscor$conf.int[1],3)
    myhb[1,"all"]=round(thiscor$conf.int[2],3)
    myn[1,"all"]=round(thiscor$parameter,3)
    myp[1,"all"]=round(thiscor$p.value,3)
    
    # repeat whole for cp
    
    lims=range(c(proportions[,"z_cp"],proportions[,"l_cp"]))

  plot(proportions[,"z_cp"]~proportions[,"l_cp"], xlab=prettynames["l_cp"],ylab=prettynames["z_cp"],
       xlim=lims,ylim=lims,
       col=mycols[proportions$Diagnosis], pch=mypch[proportions$Diagnosis])
  lines(c(0,1),c(0,1),lty=2,col="darkgray")

  abline(lm(proportions[,"z_cp"]~proportions[,"l_cp"],subset=c(proportions$Diagnosis=="AngelmanSyndrome")),col="black")

  #get cor properties
  thiscor=cor.test(proportions[proportions$Diagnosis=="AngelmanSyndrome","z_cp"],proportions[proportions$Diagnosis=="AngelmanSyndrome","l_cp"])
    myr[2,"AS"]=round(thiscor$estimate,3)
    mylb[2,"AS"]=round(thiscor$conf.int[1],3)
    myhb[2,"AS"]=round(thiscor$conf.int[2],3)
    myn[2,"AS"]=round(thiscor$parameter,3)
    myp[2,"AS"]=round(thiscor$p.value,3)
    

  #repeat for LR  
    abline(lm(proportions[,"z_cp"]~proportions[,"l_cp"],subset=c(proportions$Diagnosis!="AngelmanSyndrome")),col="red")

      #get cor properties
  thiscor=cor.test(proportions[proportions$Diagnosis!="AngelmanSyndrome","l_cp"],proportions[proportions$Diagnosis!="AngelmanSyndrome","z_cp"])
    myr[2,"LR"]=round(thiscor$estimate,3)
    mylb[2,"LR"]=round(thiscor$conf.int[1],3)
    myhb[2,"LR"]=round(thiscor$conf.int[2],3)
    myn[2,"LR"]=round(thiscor$parameter,3)
    myp[2,"LR"]=round(thiscor$p.value,3)
    
#repeat for all cor
    
      thiscor=cor.test(proportions$z_cp,proportions$l_cp)
    myr[2,"all"]=round(thiscor$estimate,3)
    mylb[2,"all"]=round(thiscor$conf.int[1],3)
    myhb[2,"all"]=round(thiscor$conf.int[2],3)
    myn[2,"all"]=round(thiscor$parameter,3)
    myp[2,"all"]=round(thiscor$p.value,3)
    
    

```


```{r fig-cor-Junk, echo=F,fig.cap ="Correlations between child-level descriptors derived from Zooniverse data (y axis) versus lab data (x axis) as a function of whether children's Junk proportion was higher or lower than the median for all children (light blue wheels = lower, and green diamonds = higher)."}

mycols=c("lightblue","darkgreen")
mypch=c(10,18)
names(mycols)=names(mypch)=c("lower_Junk","higher_Junk")

layout(matrix(c(1:2), 1, 2, byrow = F))
par(mar = c(4, 4, 1, 1))

#create matrix to hold r results
myr=matrix(NA,nrow=2,ncol=2)
rownames(myr)<-c("lp","cp")
colnames(myr)<-c("lower_Junk","higher_Junk")
mylb=myhb=myn=myp =myr #this is an ugly way of creating 5 matrices

lims=range(c(proportions[,"z_lp"],proportions[,"l_lp"]))

  plot(proportions[,"z_lp"]~proportions[,"l_lp"], xlab=prettynames["l_lp"],ylab=prettynames["z_lp"],
       xlim=lims,ylim=lims,
       col=mycols[proportions$median_Junk], pch=mypch[proportions$median_Junk])
  lines(c(0,1),c(0,1),lty=2,col="darkgray")

  abline(lm(proportions[,"z_lp"]~proportions[,"l_lp"],subset=c(proportions$median_Junk=="lower_Junk")),col="lightblue")

  #get cor properties
  thiscor=cor.test(proportions[proportions$median_Junk=="lower_Junk","z_lp"],proportions[proportions$median_Junk=="lower_Junk","l_lp"])
    myr[1,"lower_Junk"]=round(thiscor$estimate,3)
    mylb[1,"lower_Junk"]=round(thiscor$conf.int[1],3)
    myhb[1,"lower_Junk"]=round(thiscor$conf.int[2],3)
    myn[1,"lower_Junk"]=round(thiscor$parameter,3)
    myp[1,"lower_Junk"]=round(thiscor$p.value,3)
    

  #repeat for LR  
    abline(lm(proportions[,"z_lp"]~proportions[,"l_lp"],subset=c(proportions$median_Junk!="lower_Junk")),col="darkgreen")

      #get cor properties
  thiscor=cor.test(proportions[proportions$median_Junk!="lower_Junk","l_lp"],proportions[proportions$median_Junk!="lower_Junk","z_lp"])
    myr[1,"higher_Junk"]=round(thiscor$estimate,3)
    mylb[1,"higher_Junk"]=round(thiscor$conf.int[1],3)
    myhb[1,"higher_Junk"]=round(thiscor$conf.int[2],3)
    myn[1,"higher_Junk"]=round(thiscor$parameter,3)
    myp[1,"higher_Junk"]=round(thiscor$p.value,3)
    

    
    # repeat whole for cp
    
    lims=range(c(proportions[,"z_cp"],proportions[,"l_cp"]))

  plot(proportions[,"z_cp"]~proportions[,"l_cp"], xlab=prettynames["l_cp"],ylab=prettynames["z_cp"],
       xlim=lims,ylim=lims,
       col=mycols[proportions$median_Junk], pch=mypch[proportions$median_Junk])
  lines(c(0,1),c(0,1),lty=2,col="darkgray")

  abline(lm(proportions[,"z_cp"]~proportions[,"l_cp"],subset=c(proportions$median_Junk=="lower_Junk")),col="lightblue")

  #get cor properties
  thiscor=cor.test(proportions[proportions$median_Junk=="lower_Junk","z_cp"],proportions[proportions$median_Junk=="lower_Junk","l_cp"])
    myr[2,"lower_Junk"]=round(thiscor$estimate,3)
    mylb[2,"lower_Junk"]=round(thiscor$conf.int[1],3)
    myhb[2,"lower_Junk"]=round(thiscor$conf.int[2],3)
    myn[2,"lower_Junk"]=round(thiscor$parameter,3)
    myp[2,"lower_Junk"]=round(thiscor$p.value,3)
    

  #repeat for higher_Junk  
    abline(lm(proportions[,"z_cp"]~proportions[,"l_cp"],subset=c(proportions$median_Junk!="lower_Junk")),col="darkgreen")

      #get cor properties
  thiscor=cor.test(proportions[proportions$median_Junk!="lower_Junk","l_cp"],proportions[proportions$median_Junk!="lower_Junk","z_cp"])
    myr[2,"higher_Junk"]=round(thiscor$estimate,3)
    mylb[2,"higher_Junk"]=round(thiscor$conf.int[1],3)
    myhb[2,"higher_Junk"]=round(thiscor$conf.int[2],3)
    myn[2,"higher_Junk"]=round(thiscor$parameter,3)
    myp[2,"higher_Junk"]=round(thiscor$p.value,3)
    

```




```{r tab-LENA-challenge, comment = F, message = F, hide = T, warning = F, echo = F, results = "asis"}
mycaption="Comparison of recall percentages obtained with the baseline algorithm created by the ComParE team, in the context of the ComParE 2019 BabySounds subchallenge (Schuller et al., 2019; C2019B), LENA labels (note LENA did not distinguish between Canonical and Non-canonical in this analysis), and those obtained in this study through Zooniverse annotations. Label frequency indicates the prevalence of the relevant label (Crying, Laughing, etc.) in each dataset (the same Lab dataset was used for both LENA and Zooniverse). UAR stands for unweighted average recall, WAR for weighted average recall (which takes into account label frequency)."

read.table("tab-lena-challenge.txt",header=T,sep ="\t")->tab1

#add zoo & lab data
recalls=round(stall_r$rec[stall_r$Lab==stall_r$Zooniverse],1)
names(recalls)<-c("Canonical","Crying","Junk","Laughing","Non-can.")
recalls=recalls[as.character(tab1[2:6,"X"])]
freqs=table(data_all$Zoon)
names(freqs)<-c("Canonical","Crying","Junk","Laughing","Non-can.")
freqs=freqs[as.character(tab1[2:6,"X"])]
w=freqs/sum(freqs)
UAR=round(mean(recalls),1)
WAR=round(weighted.mean(recalls,w),1)
tab1$z=c("Zoon.",recalls, UAR,WAR)
tab1$f=c("Zoon.",freqs, "","")
tab1=tab1[,c("X","Recall","x","z","Label.frequency","f")]
write.csv(tab1,"tab1-final.csv",row.names=F)
#olnames(tab1)<-c("","Recall","Recall","Recall","Label Frequency","Label Frequency")

# as_hux(tab1) %>% 
#     insert_row("","Recall","","","Label Frequency","", fill = "") %>% 
#     merge_cells(1, 2:4) %>% 
#     merge_cells(1, 5:6) %>%  
#     merge_cells(2, 5:6) %>%  
#     set_header_rows(1, TRUE) %>% 
#    style_headers(bold = TRUE)

kable(tab1,cap =mycaption, col.names = NULL) 
#%>%
#  collapse_rows(columns = 1:6, valign = "middle") #%>%
#add_header_above(c(" " = 1, "Recall"= 3, "Label Frequency" = 2))
```


```{r tab-cors, results="as.is"}
colnames(mycortab)<-c("Linguistic Proportion","Canonical Proportion")

mycortab_sorted= rbind(mycortab[grep("AS",rownames(mycortab)),],
                       mycortab[grep("LR",rownames(mycortab)),],
                       mycortab[grep("all",rownames(mycortab)),]
)

kable(mycortab_sorted,cap ="Pearson correlation coefficients across metrics derived from laboratory and Zooniverse annotations in the Angelman syndrome (AS) group data, low-risk (LR) group data, or for all children together (all) in three ways. First, All seg indicates that Zooniverse annotations at the chunk level were first combined at the segment level. Second, Chunks indicates that they were analyzed directly at the chunk level. Both of these are based on all the data. Third, rows labeled 100 seg indicate that laboratory- and Zooniverse-derived metrics were based on only 100 segments (median over 50 runs in which 100 segments were randomly selected from each child).")
write.csv(mycortab_sorted,"mycortab_sorted.csv")
```


```{r tab-corage, results="as.is"}

kable(corage,cap ="Pearson correlation coefficients between derived metrics (Linguistic Proportion = LP; Canonical Proportion = CP) and age, for the Angelman syndrome (AS) group data and low-risk (LR) group data. Metrics were derived from laboratory annotations at the segment level (lab); from Zooniverse annotations collapsed at the segment level (Zoon. Seg); or directly from the chunk-level labels (Zoon. Chunks). ")
write.csv(corage,"mycorage.csv")
```



# References

<!-- These lines ensure references are set with hanging indents in PDF documents; they are ignored in Word. -->
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}