---
title: "Analyses for JSLHR version"
date: "2020-10-17"
output:
  pdf_document:
    toc: yes
    toc_depth: 3
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width=12, fig.height=8) 
library(tidyverse)
library(dplyr)
library(ggplot2)
library(caret)
library(scales)
library(kableExtra)
library(rel)
library(ggpubr)
library(lme4)

```


## History:

- 2020-08-05 final first version
- 2020-10-10 (rehaul), 
- latest minor edits 2020-10-17


## TODO:

- pipeline is still not transparent
- there are duplicate files across raw and derived data
- there are a bunch of files with similar names
- README is old
- a note said read demo data created by AC from info in paper - should be replaced with real demo data
- add code to print out the results paragraphs
- corage: - adjust margins and remove title repetition?
- chunk-based analyses for age

## Read data in

```{r}
# read datasets

data_ang <- read.csv("../Derived_Data/classifications_PU_zoon_final17.csv",header=T,sep=",")
data_td <- read.csv("../Derived_Data/classifications_PU_zoon_final.csv")
data_all<-rbind(data_ang, data_td)


#remove the word mixed that takes up space and is unnecessary
data_all$Zoon_classif=factor(gsub("Mixed_","",as.character(data_all$Zoon_classif),fixed=T))
#relevel the factor so that it's easier to read
data_all$Zoon_classif=factor(data_all$Zoon_classif, levels=c("Canonical","Non-Canonical",                   "Crying","Laughing","Junk",levels(data_all$Zoon_classif)[grep("_",levels(data_all$Zoon_classif))]))
# create lab column with easier to read correspondance
data_all$lab<-as.character(data_all$Major_Choice)
data_all$lab[data_all$lab=="Non-canonical syllables"]<-"Non-Canonical"
data_all$lab[data_all$lab=="Canonical syllables"]<-"Canonical"
data_all$lab[data_all$lab %in% c("Don't mark","None")]<-"Junk"
data_all$lab=factor(data_all$lab,levels=levels(data_all$Zoon_classif))
#apply same factor levels as zooniverse so that we can do symmetrical confusion matrices

#add binomials for linguistic proportion
data_all$lab_ling=ifelse(data_all$lab %in% c("Canonical","Non-Canonical"),1,0)
data_all$zoo_ling=ifelse(data_all$Zoon_classif %in% c("Canonical","Non-Canonical"),1,0)
data_all$lab_ling[data_all$lab=="Junk"]<-NA
data_all$zoo_ling[data_all$lab=="Junk"]<-NA

#add binomials for canonical proportion
data_all$lab_can=data_all$zoo_can=NA
data_all$lab_can[data_all$lab=="Canonical"]<-1
data_all$lab_can[data_all$lab=="Non-Canonical"]<-0
data_all$zoo_can[data_all$Zoon_classif=="Canonical"]<-1
data_all$zoo_can[data_all$Zoon_classif=="Non-Canonical"]<-0


demo_data=read.csv("../Derived_Data/demo-data.tsv",sep="\t")
#add filenames to demo data, to be used later
demo_data_fn <- demo_data %>% 
     left_join(select(data_all, filename, ChildID), by = c("ChildID"))
demo_data_fn<-unique(demo_data_fn)

```


## Data post-processing

We collected a total of 169,628 judgments provided for 33,880 500-ms chunks, corresponding to 11,984 LENA segments. Nearly a fifth of chunks did not have at least 3 labels in agreement out of the 5 Zooniverse labels (N = 6,585, 19% of all chunks). Of the chunks without a majority agreement, 4341 (66%) contained one or two Junk judgements (out of 5), 6523 (99,9%) had at least two matching judgements (the threshold used for lab-annotated segments), and only 61 (0,01%) had 5 different judgements. Future work may explore different ways of setting the minimal requirement for convergence, but for further analyses here, we focused on the 81% of chunks that did have at least 3 labels in agreement; this represented 136,703 labels for 27,295 chunks, corresponding to 11,593 LENA segments. As the segments average 1.12 seconds in length, this means about 3.8 hours of audio data were annotated by 8 different annotators (3 in the laboratory, 5 on Zooniverse). 



```{r}
# remove non-majority labels
#TODO 

# we map the mixed
data_all$Zoon_classif[data_all$Zoon_classif=="Laughing_Canonical"]<-"Canonical"
data_all$Zoon_classif[data_all$Zoon_classif=="Laughing_Non-Canonical"]<-"Non-Canonical"
data_all$Zoon_classif[data_all$Zoon_classif=="Laughing_Non-Canonical_Crying"]<-"Non-Canonical"
data_all$Zoon_classif[data_all$Zoon_classif=="Laughing_Crying"]<-"Crying"
data_all$Zoon_classif[data_all$Zoon_classif=="Non-Canonical_Crying"]<-"Non-Canonical"
data_all$Zoon_classif[data_all$Zoon_classif=="Non-Canonical_Laughing_Crying"]<-"Non-Canonical"
data_all$Zoon_classif[data_all$Zoon_classif=="Crying_Canonical"]<-"Canonical"
# +
data_all$Zoon_classif[data_all$Zoon_classif=="Canonical_Crying"]<-"Canonical"
data_all$Zoon_classif[data_all$Zoon_classif=="Canonical_Laughing"]<-"Canonical"
data_all$Zoon_classif[data_all$Zoon_classif=="Laughing_Canonical_Crying"]<-"Non-Canonical"

data_all$Zoon_classif[data_all$Zoon_classif=="Crying_Laughing"]<-"Crying"
data_all$Zoon_classif[data_all$Zoon_classif=="Crying_Canonical_Laughing"]<-"Canonical"
data_all$Zoon_classif[data_all$Zoon_classif=="Crying_Laughing_Non-Canonical"]<-"Non-Canonical"
data_all$Zoon_classif[data_all$Zoon_classif=="Crying_Non-Canonical"]<-"Non-Canonical"
data_all$Zoon_classif[data_all$Zoon_classif=="Crying_Non-Canonical_Laughing"]<-"Non-Canonical" 
data_all$Zoon_classif[data_all$Zoon_classif=="Non-Canonical_Laughing"]<-"Non-Canonical"

#and reset the factors for cleanliness
data_all$Zoon_classif=factor(data_all$Zoon_classif)
data_all$lab=factor(data_all$lab)
sample_data<-cbind(data_all$lab,data_all$Zoon_classif)

table_zoon=round(table(data_all$Zoon)/sum(table(data_all$Zoon))*100)
table_lab=round(table(data_all$lab)/sum(table(data_all$lab))*100)
```

```{r prepare data by child}

#get the ns by child, then calculate the linguistic ratio & canonical ratio, separately for zooniverse & lab
ztab=table(data_all$filename,data_all$Zoon_classif)
z_lp=rowSums(ztab[,c("Canonical","Non-Canonical")])/rowSums(ztab[,-which(colnames(ztab) %in% c("Junk"))])
z_cp=ztab[,c("Canonical")]/rowSums(ztab[,c("Canonical","Non-Canonical")])

ltab=table(data_all$filename,data_all$lab)
l_lp=rowSums(ltab[,c("Canonical","Non-Canonical")])/rowSums(ltab[,-which(colnames(ztab) %in% c("Junk"))])
l_cp=ltab[,c("Canonical")]/rowSums(ltab[,c("Canonical","Non-Canonical")])

#and also get junk from zooniverse
z_junk=ztab[,c("Junk")]/rowSums(ztab)

  
#put all the proportions together
if(sum(rownames(ztab)==rownames(ltab))==dim(ztab)[1]) proportions=cbind(rownames(ztab),z_lp,z_cp,l_lp,l_cp,z_junk) else print("oops this code needs to be more complex because we don't have the same kids for the two proportions")
colnames(proportions)[1]<-"filename"

#ages=aggregate(data_all$Age,by=list(data_all$ChildID),mean) #this is a weird way of adding ages, since all of the ages for a given child should be the same if there is only one recording, and if there are multiple recordings, then we should not get the mean
#improvement: now we merge with a demo data tab, but note this is merged with child id, so the problem of multiple recs per child is still there

# Created demo_data with filenames. Use filenames instead of childIDs to merge proportions and demo data.

merge(proportions,demo_data_fn,by="filename")->proportions
colnames(proportions)[dim(proportions)[2]]<-"Age"

#cbinding results in text, so we numerize the proportions
for(thisvar in c("z_lp","z_cp","l_lp","l_cp","z_junk")) proportions[,thisvar]=as.numeric(as.character(proportions[,thisvar]))

#add median junk
proportions$median_junk<-ifelse(proportions$z_junk<median(proportions$z_junk),"lower_junk","higher_junk")

summary(proportions)

merge(data_all,proportions,all=T)->data_all
```
## Results

### Descriptive analyses

In this section, we provide descriptive analyses of our dataset. According to lab annotators, `r table_lab["Canonical"]`% of segments were canonical, `r table_lab["Non-Canonical"]`% non-canonical, `r table_lab["Laughing"]`% laughing, and `r table_lab["Crying"]`% crying, with the remaining `r table_lab["Junk"]`% being categorized as "Don't code". Zooniverse data revealed a similar distribution:  `r table_zoon["Canonical"]`% canonical, `r table_zoon["Non-Canonical"]`% non-canonical, `r table_zoon["Laughing"]`% laughing, `r table_zoon["Crying"]`% crying, `r table_zoon["Junk"]`% junk.
Next, we inspected the relationship between age and child-level derived metrics, of which we had two: i) Linguistic proportion = (“Canonical”+“Non-Canonical”)/“All vocalizations” (i.e., we remove junk), and ii) Canonical proportion = “Canonical”/(“Canonical”+“Non-Canonical”) (i.e. we remove junk + non-linguistic vocalizations). See Figure XX for results.

Figure XX. Correlations between child-level descriptors and age as a function of metric (linguistic ratio in the top row, canonical ratio in the bottom row), annotation method, and child group (Red as TD, and black as AS) 




```{r corage}

prettynames=c("Linguistic Proportion (Zooniverse)","Canonical Proportion (Zooniverse)",
             "Linguistic Proportion (Lab)","Canonical Proportion (Lab)" )
names(prettynames)<-c("z_lp","z_cp","l_lp","l_cp")
mycols=c("black","red")
names(mycols)<-c("Low-RiskControl","AngelmanSyndrome")
mypch=c(4,20)
names(mypch)<-c("Low-RiskControl","AngelmanSyndrome")

lp_range=range(proportions[,c("z_lp","l_lp")])
cp_range=range(proportions[,c("z_cp","l_cp")])

#jpeg("../Results/corage.jpg",width=20,height=20,units="cm",res=300)
layout(matrix(c(1:4), 2, 2, byrow = F))
par(mar = c(4, 4, 1, 1))

#create matrix to hold r results
myr=matrix(NA,nrow=4,ncol=2)
rownames(myr)<-c("z_lp","z_cp","l_lp","l_cp")
colnames(myr)<-c("AS","LR")
mylb=myhb=myn=myp=myr #this is an ugly way of creating 5 matrices

for(thisvar in c("z_lp","z_cp","l_lp","l_cp")) {

  if(thisvar %in% c("z_lp","l_lp")) myrange=lp_range else myrange=cp_range
    
  plot(proportions[,thisvar]~proportions$Age, pch=mypch[proportions$Diagnosis],xlab="Age (months)",ylab=prettynames[thisvar],
       col=mycols[proportions$Diagnosis],
         ylim=myrange)
  abline(lm(proportions[,thisvar]~proportions$Age,subset=c(proportions$Diagnosis=="AngelmanSyndrome")),col="black")

  #get cor properties
  thiscor=cor.test(proportions[proportions$Diagnosis=="AngelmanSyndrome",thisvar],proportions$Age[proportions$Diagnosis=="AngelmanSyndrome"])
    myr[thisvar,"AS"]=round(thiscor$estimate,3)
    mylb[thisvar,"AS"]=round(thiscor$conf.int[1],3)
    myhb[thisvar,"AS"]=round(thiscor$conf.int[2],3)
    myn[thisvar,"AS"]=round(thiscor$parameter,3)
    myp[thisvar,"AS"]=round(thiscor$p.value,3)
    
    #add r to figure
  text(mean(proportions$Age[proportions$Diagnosis=="AngelmanSyndrome"]),mean(proportions[proportions$Diagnosis=="AngelmanSyndrome",thisvar]),paste0("r=",myr[thisvar,"AS"]),col="black")

  #repeat for LR  
    abline(lm(proportions[,thisvar]~proportions$Age,subset=c(proportions$Diagnosis!="AngelmanSyndrome")),col="red")

      #get cor properties
  thiscor=cor.test(proportions[proportions$Diagnosis!="AngelmanSyndrome",thisvar],proportions$Age[proportions$Diagnosis!="AngelmanSyndrome"])
    myr[thisvar,"LR"]=round(thiscor$estimate,3)
    mylb[thisvar,"LR"]=round(thiscor$conf.int[1],3)
    myhb[thisvar,"LR"]=round(thiscor$conf.int[2],3)
    myn[thisvar,"LR"]=round(thiscor$parameter,3)
    myp[thisvar,"LR"]=round(thiscor$p.value,3)
    
    #add r to figure
    
    text(mean(proportions$Age[proportions$Diagnosis!="AngelmanSyndrome"]),mean(proportions[proportions$Diagnosis!="AngelmanSyndrome",thisvar]),paste0("r=",myr[thisvar,"LR"]),col="red")
}
#dev.off()
```




Descriptive analyses on the laboratory annotations showed that correlations between the Linguistic proportion and age differed across the groups. There was a near-zero relationship among the older children diagnosed with Angelman Syndrome r(`r myn["l_lp","AS"]`) = `r myr["l_lp","AS"]`, CI [`r mylb["l_lp","AS"]`,`r myhb["l_lp","AS"]`], p=`r myp["l_lp","AS"]`]; and a significant association among younger low-risk control children r(`r myn["l_lp","LR"]`) = `r myr["l_lp","LR"]`, CI [`r mylb["l_lp","LR"]`,`r myhb["l_lp","LR"]`], p=`r myp["l_lp","LR"]`]. The Canonical proportion exhibited non-significant developmental decreases among older children diagnosed with Angelman Syndrome r(`r myn["l_cp","AS"]`) = `r myr["l_cp","AS"]`, CI [`r mylb["l_cp","AS"]`,`r myhb["l_cp","AS"]`], p=`r myp["l_cp","AS"]`]; and marginal developmental increases among low-risk control r(`r myn["l_cp","LR"]`) = `r myr["l_cp","LR"]`, CI [`r mylb["l_cp","LR"]`,`r myhb["l_cp","LR"]`], p=`r myp["l_cp","LR"]`].


Using the Zooniverse annotations, we found that the association with age was very weak for children diagnosed with Angelman Syndrome r(`r myn["z_lp","AS"]`) = `r myr["z_lp","AS"]`, CI [`r mylb["z_lp","AS"]`,`r myhb["z_lp","AS"]`], p=`r myp["z_lp","AS"]`]; whereas younger low-risk control children showed a significant increase with age r(`r myn["z_lp","LR"]`) = `r myr["z_lp","LR"]`, CI [`r mylb["z_lp","LR"]`,`r myhb["z_lp","LR"]`], p=`r myp["z_lp","LR"]`]. Similarly, there were non-significant developmental decreases in the Canonical among children with Angelman Syndrome r(`r myn["z_cp","AS"]`) = `r myr["z_cp","AS"]`, CI [`r mylb["z_cp","AS"]`,`r myhb["z_cp","AS"]`], p=`r myp["z_cp","AS"]`]; and marginal developmental increases among low-risk control children, r(`r myn["z_cp","LR"]`) = `r myr["z_cp","LR"]`, CI [`r mylb["z_cp","LR"]`,`r myhb["z_cp","LR"]`], p=`r myp["z_cp","LR"]`].
```{r bychunk-age}
read.csv("../Derived_Data/chunk_majority_agreement_1751.csv")->chunk
bychild=table(chunk$Answer,chunk$ChildID)
cans=bychild["Canonical",]
noncans=bychild["Non-Canonical",]
junk=bychild["Junk",]
sum_bychild=table(chunk$ChildID)

ling_prop=(cans+noncans)/(sum_bychild-junk)
can_prop=cans/(cans+noncans)

demo_data$ChildID==names(ling_prop)
demo_data$ChildID==names(can_prop)

props_bychild_chunk=cbind(ling_prop,can_prop,demo_data)
colnames(props_bychild_chunk)[1:2]<-c("chi","ling_prop")

z_lp_as=cor.test(props_bychild_chunk$ling_prop[props_bychild_chunk$Diagnosis=="AngelmanSyndrome"],props_bychild_chunk$Age[props_bychild_chunk$Diagnosis=="AngelmanSyndrome"])

z_lp_lr=cor.test(props_bychild_chunk$ling_prop[props_bychild_chunk$Diagnosis!="AngelmanSyndrome"],props_bychild_chunk$Age[props_bychild_chunk$Diagnosis!="AngelmanSyndrome"])

z_cp_as=cor.test(props_bychild_chunk$can_prop[props_bychild_chunk$Diagnosis=="AngelmanSyndrome"],props_bychild_chunk$Age[props_bychild_chunk$Diagnosis=="AngelmanSyndrome"])

z_cp_lr=cor.test(props_bychild_chunk$can_prop[props_bychild_chunk$Diagnosis!="AngelmanSyndrome"],props_bychild_chunk$Age[props_bychild_chunk$Diagnosis!="AngelmanSyndrome"])
```


Given that these derived metrics are defined at the child level, there was one stage in our pre-processing that may not have been necessary, whereby we collapsed judgments across chunks associated to the same segment. We therefore repeated our analyses but deriving our proportions for the Zooniverse data not from the segment-level composite, but rather the individual chunk-level annotations. We found that the Linguistic proportion was not strongly associated with age in the Angelman Syndrome group: r(`r z_lp_as$parameter`) = `r z_lp_as$estimate`, CI [`r z_lp_as$conf.int[1]`,`r z_lp_as$conf.int[2]`], p=`r z_lp_as$p.value`], but increased with age  in the low-risk control group: r(`r z_lp_lr$parameter`) = `r z_lp_lr$estimate`, CI [`r z_lp_lr$conf.int[1]`,`r z_lp_lr$conf.int[2]`], p=`r z_lp_lr$p.value`. Similarly, the Canonical proportion did not exhibit the same pattern across the groups, with developmental decreases found among children with Angelman Syndrome r(`r z_cp_as$parameter`) = `r z_cp_as$estimate`, CI [`r z_cp_as$conf.int[1]`,`r z_cp_as$conf.int[2]`], p=`r z_cp_as$p.value`]; and developmental increases among low-risk control r(`r z_cp_lr$parameter`) = `r z_cp_lr$estimate`, CI [`r z_cp_lr$conf.int[1]`,`r z_cp_lr$conf.int[2]`], p=`r z_cp_lr$p.value`.


## Main analyses

Next, we discuss the correspondence between citizen science classifications and the laboratory gold standard, at the level of individual clips. Results were visualized and assessed with a confusion matrix. We report a Precision plot (Fig. 1) and a Recall plot (Fig. 2): the diagonal elements show the number of correct segment-level classifications for each class while the off-diagonal elements show non-matching classifications.

```{r mygac-conf}
mygac=gac(data = sample_data, kat = 5, weight = c("unweighted"),
    conf.level = 0.95)

mycf=confusionMatrix(data_all$lab, data_all$Zoon_classif, dnn = c("Lab","Zooniverse"))
conf_tab=mycf$table
# this package uses sensitivity & specificity
#Sensitivity=recall
#Specificity=precision
#mycf
```





```{r figure-prec-rec}


colsums=colSums(conf_tab)
my_conf_tab=conf_tab
for(i in 1:dim(conf_tab)[2]) my_conf_tab[,i]=my_conf_tab[,i]/colsums[i]
colSums(my_conf_tab)
prop_cat=data.frame(my_conf_tab*100) #generates precision because columns
prop_cat$id=paste(prop_cat$Lab,prop_cat$Zooniverse)
colnames(prop_cat)[3]<-"pr"
data.frame(conf_tab)->stall
stall$id=paste(stall$Lab,stall$Zooniverse)
stall=merge(stall,prop_cat[c("id","pr")])
prec_plot = ggplot(data = stall, mapping = aes(y = Lab, x=Zooniverse)) +
 geom_tile(aes(fill= rescale(pr)), colour = "white") +
  geom_text(aes(label = paste(round(pr),"%")), vjust = -1,size=2) +
  geom_text(aes(label = Freq), vjust = 1,size=1) +
  scale_fill_gradient(low = "white", high = "red", name = "Percentage") +
     theme(legend.position = "none") +
  xlab("Zooniverse") + ylab("Lab") +
  ggtitle("Precision")+ theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

# repeat for recall
rowsums=rowSums(conf_tab)
my_conf_tab=conf_tab
for(i in 1:dim(conf_tab)[2]) my_conf_tab[,i]=my_conf_tab[,i]/rowsums[i]
rowSums(my_conf_tab)
prop_cat=data.frame(conf_tab/rowSums(conf_tab)*100)  #generates recall because rows
prop_cat$id=paste(prop_cat$Lab,prop_cat$Zooniverse)
colnames(prop_cat)[3]<-"rec"
data.frame(conf_tab)->stall
stall$id=paste(stall$Lab,stall$Zooniverse)
stall=merge(stall,prop_cat[c("id","rec")])
rec_plot = ggplot(data = stall, mapping = aes(y = Lab, x=Zooniverse)) +
 geom_tile(aes(fill= rescale(rec)), colour = "white") +
  geom_text(aes(label = paste(round(rec),"%")), vjust = -1,size=2) +
  geom_text(aes(label = Freq), vjust = 1,size=1) +
  scale_fill_gradient(low = "white", high = "red", name = "Percentage") +
     theme(legend.position = "none") +
  xlab("Zooniverse") + ylab("Lab") +
  ggtitle("Recall")+ theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

ggarrange(prec_plot, rec_plot, 
          labels = c("A", "B"),
          ncol = 2, nrow = 1)

```


From both visualizations, it appears that performance is moderate to good, with an overall accuracy of `r round(mycf$overall["Accuracy"]*100)`%, CI = [`r round(mycf$overall["AccuracyLower"]*100)`,`r round(mycf$overall["AccuracyUpper"]*100)`], a kappa of `r round(mycf$overall["Kappa"],3)`, and a Gwet's AC1 coefficient of `r round(mygac$est,3)`, CI = [`r round(mygac$lb,3)`,`r round(mygac$ub,3)`]. 



### Child level descriptors



```{r corlab-zoo}

layout(matrix(c(1:2), 1, 2, byrow = F))
par(mar = c(4, 4, 1, 1))

#create matrix to hold r results
myr=matrix(NA,nrow=2,ncol=3)
rownames(myr)<-c("lp","cp")
colnames(myr)<-c("AS","LR","all")
mylb=myhb=myn=myp=myr #this is an ugly way of creating 5 matrices

lims=range(c(proportions[,"z_lp"],proportions[,"l_lp"]))

  plot(proportions[,"z_lp"]~proportions[,"l_lp"], xlab=prettynames["l_lp"],ylab=prettynames["z_lp"],
       xlim=lims,ylim=lims,
       col=mycols[proportions$Diagnosis], pch=mypch[proportions$Diagnosis])
  lines(c(0,1),c(0,1),lty=2,col="darkgray")

  abline(lm(proportions[,"z_lp"]~proportions[,"l_lp"],subset=c(proportions$Diagnosis=="AngelmanSyndrome")),col="black")

  #get cor properties
  thiscor=cor.test(proportions[proportions$Diagnosis=="AngelmanSyndrome","z_lp"],proportions[proportions$Diagnosis=="AngelmanSyndrome","l_lp"])
    myr[1,"AS"]=round(thiscor$estimate,3)
    mylb[1,"AS"]=round(thiscor$conf.int[1],3)
    myhb[1,"AS"]=round(thiscor$conf.int[2],3)
    myn[1,"AS"]=round(thiscor$parameter,3)
    myp[1,"AS"]=round(thiscor$p.value,3)
    

  #repeat for LR  
    abline(lm(proportions[,"z_lp"]~proportions[,"l_lp"],subset=c(proportions$Diagnosis!="AngelmanSyndrome")),col="red")

      #get cor properties
  thiscor=cor.test(proportions[proportions$Diagnosis!="AngelmanSyndrome","l_lp"],proportions[proportions$Diagnosis!="AngelmanSyndrome","z_lp"])
    myr[1,"LR"]=round(thiscor$estimate,3)
    mylb[1,"LR"]=round(thiscor$conf.int[1],3)
    myhb[1,"LR"]=round(thiscor$conf.int[2],3)
    myn[1,"LR"]=round(thiscor$parameter,3)
    myp[1,"LR"]=round(thiscor$p.value,3)
    
#repeat for all cor
    
      thiscor=cor.test(proportions$z_lp,proportions$l_lp)
    myr[1,"all"]=round(thiscor$estimate,3)
    mylb[1,"all"]=round(thiscor$conf.int[1],3)
    myhb[1,"all"]=round(thiscor$conf.int[2],3)
    myn[1,"all"]=round(thiscor$parameter,3)
    myp[1,"all"]=round(thiscor$p.value,3)
    
    # repeat whole for cp
    
    lims=range(c(proportions[,"z_cp"],proportions[,"l_cp"]))

  plot(proportions[,"z_cp"]~proportions[,"l_cp"], xlab=prettynames["l_cp"],ylab=prettynames["z_cp"],
       xlim=lims,ylim=lims,
       col=mycols[proportions$Diagnosis], pch=mypch[proportions$Diagnosis])
  lines(c(0,1),c(0,1),lty=2,col="darkgray")

  abline(lm(proportions[,"z_cp"]~proportions[,"l_cp"],subset=c(proportions$Diagnosis=="AngelmanSyndrome")),col="black")

  #get cor properties
  thiscor=cor.test(proportions[proportions$Diagnosis=="AngelmanSyndrome","z_cp"],proportions[proportions$Diagnosis=="AngelmanSyndrome","l_cp"])
    myr[2,"AS"]=round(thiscor$estimate,3)
    mylb[2,"AS"]=round(thiscor$conf.int[1],3)
    myhb[2,"AS"]=round(thiscor$conf.int[2],3)
    myn[2,"AS"]=round(thiscor$parameter,3)
    myp[2,"AS"]=round(thiscor$p.value,3)
    

  #repeat for LR  
    abline(lm(proportions[,"z_cp"]~proportions[,"l_cp"],subset=c(proportions$Diagnosis!="AngelmanSyndrome")),col="red")

      #get cor properties
  thiscor=cor.test(proportions[proportions$Diagnosis!="AngelmanSyndrome","l_cp"],proportions[proportions$Diagnosis!="AngelmanSyndrome","z_cp"])
    myr[2,"LR"]=round(thiscor$estimate,3)
    mylb[2,"LR"]=round(thiscor$conf.int[1],3)
    myhb[2,"LR"]=round(thiscor$conf.int[2],3)
    myn[2,"LR"]=round(thiscor$parameter,3)
    myp[2,"LR"]=round(thiscor$p.value,3)
    
#repeat for all cor
    
      thiscor=cor.test(proportions$z_cp,proportions$l_cp)
    myr[2,"all"]=round(thiscor$estimate,3)
    mylb[2,"all"]=round(thiscor$conf.int[1],3)
    myhb[2,"all"]=round(thiscor$conf.int[2],3)
    myn[2,"all"]=round(thiscor$parameter,3)
    myp[2,"all"]=round(thiscor$p.value,3)
```



Although the classification at the clip level is only moderately accurate, what we are ultimately interested in is whether citizen scientists’ classifications are able to provide a reliable snapshot of childrens’ individual development. Looking at all 20 children together, we found a strong positive correlation r(`r myn["lp","all"]`) = `r myr["lp","all"]`, CI [`r mylb["lp","all"]`,`r myhb["lp","all"]`], p=`r myp["lp","all"]`] between Linguistic proportion by child from the Zooniverse and the lab annotators' data.  When we split by participant group, correlations remain high: for Angelman Syndrome r(`r myn["lp","AS"]`) = `r myr["lp","AS"]`, CI [`r mylb["lp","AS"]`,`r myhb["lp","AS"]`], p=`r myp["lp","AS"]`]; low-risk control r(`r myn["lp","LR"]`) = `r myr["lp","LR"]`, CI [`r mylb["lp","LR"]`,`r myhb["lp","LR"]`], p=`r myp["lp","LR"]`]. 

Similarly, a strong positive correlation is found in the Canonical Proportion r(`r myn["cp","all"]`) = `r myr["cp","all"]`, CI [`r mylb["cp","all"]`,`r myhb["cp","all"]`], p=`r myp["cp","all"]`]. When we split by participant group, correlations remain high although we do note they are somewhat smaller for the older children with Angelman Syndrome: r(`r myn["cp","AS"]`) = `r myr["cp","AS"]`, CI [`r mylb["cp","AS"]`,`r myhb["cp","AS"]`], p=`r myp["cp","AS"]`]; than the low-risk control children r(`r myn["cp","LR"]`) = `r myr["cp","LR"]`, CI [`r mylb["cp","LR"]`,`r myhb["cp","LR"]`], p=`r myp["cp","LR"]`.

```{r zoo-lab-chunk}

#create matrix to hold r results
myr=matrix(NA,nrow=2,ncol=3)
rownames(myr)<-c("lp","cp")
colnames(myr)<-c("AS","LR","all")
mylb=myhb=myn=myp=myr #this is an ugly way of creating 5 matrices

proportions=proportions[order(proportions$ChildID),]

  #get cor properties
  thiscor=cor.test(props_bychild_chunk[props_bychild_chunk$Diagnosis=="AngelmanSyndrome","ling_prop"],proportions[proportions$Diagnosis=="AngelmanSyndrome","l_lp"])
    myr[1,"AS"]=round(thiscor$estimate,3)
    mylb[1,"AS"]=round(thiscor$conf.int[1],3)
    myhb[1,"AS"]=round(thiscor$conf.int[2],3)
    myn[1,"AS"]=round(thiscor$parameter,3)
    myp[1,"AS"]=round(thiscor$p.value,3)
    

  #repeat for LR  


      #get cor properties
  thiscor=cor.test(props_bychild_chunk[props_bychild_chunk$Diagnosis!="AngelmanSyndrome","ling_prop"],proportions[proportions$Diagnosis!="AngelmanSyndrome","l_lp"])
    myr[1,"LR"]=round(thiscor$estimate,3)
    mylb[1,"LR"]=round(thiscor$conf.int[1],3)
    myhb[1,"LR"]=round(thiscor$conf.int[2],3)
    myn[1,"LR"]=round(thiscor$parameter,3)
    myp[1,"LR"]=round(thiscor$p.value,3)
    
#repeat for all cor
    
      thiscor=cor.test(props_bychild_chunk$ling_prop,proportions$l_lp)
    myr[1,"all"]=round(thiscor$estimate,3)
    mylb[1,"all"]=round(thiscor$conf.int[1],3)
    myhb[1,"all"]=round(thiscor$conf.int[2],3)
    myn[1,"all"]=round(thiscor$parameter,3)
    myp[1,"all"]=round(thiscor$p.value,3)
    
    # repeat whole for cp


  #get cor properties
  thiscor=cor.test(props_bychild_chunk[props_bychild_chunk$Diagnosis=="AngelmanSyndrome","can_prop"],proportions[proportions$Diagnosis=="AngelmanSyndrome","l_cp"])
    myr[2,"AS"]=round(thiscor$estimate,3)
    mylb[2,"AS"]=round(thiscor$conf.int[1],3)
    myhb[2,"AS"]=round(thiscor$conf.int[2],3)
    myn[2,"AS"]=round(thiscor$parameter,3)
    myp[2,"AS"]=round(thiscor$p.value,3)
    

  #repeat for LR  


      #get cor properties
  thiscor=cor.test(props_bychild_chunk[props_bychild_chunk$Diagnosis!="AngelmanSyndrome","can_prop"],proportions[proportions$Diagnosis!="AngelmanSyndrome","l_cp"])
    myr[2,"LR"]=round(thiscor$estimate,3)
    mylb[2,"LR"]=round(thiscor$conf.int[1],3)
    myhb[2,"LR"]=round(thiscor$conf.int[2],3)
    myn[2,"LR"]=round(thiscor$parameter,3)
    myp[2,"LR"]=round(thiscor$p.value,3)
    
#repeat for all cor
    
      thiscor=cor.test(props_bychild_chunk$can_prop,proportions$l_cp)
    myr[2,"all"]=round(thiscor$estimate,3)
    mylb[2,"all"]=round(thiscor$conf.int[1],3)
    myhb[2,"all"]=round(thiscor$conf.int[2],3)
    myn[2,"all"]=round(thiscor$parameter,3)
    myp[2,"all"]=round(thiscor$p.value,3)
```


When we use chunk-level judgments to derive the our child-level metrics, we observe very similar levels of correlation: linguistic proportion overall r(`r myn["lp","all"]`) = `r myr["lp","all"]`, CI [`r mylb["lp","all"]`,`r myhb["lp","all"]`], p=`r myp["lp","all"]`] between Linguistic proportion by child from the Zooniverse and the lab annotators' data.  When we split by participant group, correlations remain high: for Angelman Syndrome r(`r myn["lp","AS"]`) = `r myr["lp","AS"]`, CI [`r mylb["lp","AS"]`,`r myhb["lp","AS"]`], p=`r myp["lp","AS"]`]; low-risk control r(`r myn["lp","LR"]`) = `r myr["lp","LR"]`, CI [`r mylb["lp","LR"]`,`r myhb["lp","LR"]`], p=`r myp["lp","LR"]`]. As for canonical proportion overall r(`r myn["cp","all"]`) = `r myr["cp","all"]`, CI [`r mylb["cp","all"]`,`r myhb["cp","all"]`], p=`r myp["cp","all"]`; Angelman Syndrome: r(`r myn["cp","AS"]`) = `r myr["cp","AS"]`, CI [`r mylb["cp","AS"]`,`r myhb["cp","AS"]`], p=`r myp["cp","AS"]`];  low-risk control infants r(`r myn["cp","LR"]`) = `r myr["cp","LR"]`, CI [`r mylb["cp","LR"]`,`r myhb["cp","LR"]`], p=`r myp["cp","LR"]`.






```{r glmer-agediag}
# one outlier in ling prop is removed

ling_all=glmer(lab_ling~zoo_ling*Diagnosis*Age + (1|ChildID),data=data_all,family="binomial",subset=c(ChildID!="3681_1"))
#plot(lin_mod)
# doesn't converge

ling_AS=glmer(lab_ling~zoo_ling*Age + (1|ChildID),data=data_all,family="binomial",subset=c(Diagnosis=="AngelmanSyndrome"))
#only effect of zoo

ling_LR=glmer(lab_ling~zoo_ling*Age + (1|ChildID),data=data_all,family="binomial",subset=c(Diagnosis!="AngelmanSyndrome" & ChildID!="3681_1"))
#both main and interaction are sig


can_all=glmer(lab_can~zoo_can*Diagnosis*Age + (1|ChildID),data=data_all,family="binomial")
#doesn't converge either

can_AS=glmer(lab_can~zoo_can*Age + (1|ChildID),data=data_all,family="binomial",subset=c(Diagnosis=="AngelmanSyndrome"))
#only effect of zoo

can_LR=glmer(lab_can~zoo_can*Age + (1|ChildID),data=data_all,family="binomial",subset=c(Diagnosis!="AngelmanSyndrome"))
#both main and interaction are sig

```

## Additional analyses

We additionally explored under what conditions Zooniverse judgments more closely aligned with laboratory judgments. In previous work using a similar method, for instance, data from all three children from one dataset were often labeled as "Junk" (i.e., not a child's vocalization), and the data points from this corpus stood out when the authors attempted to integrate results with other corpora [@cychosz]. A high proportion of "Junk" may indicate that automated segmentation was errorful for those children, and may be a sign that the rest of the data could be compromised as well. 

```{r prelim-junk}
junkXdiag=t.test(proportions$z_junk ~ proportions$Diagnosis)

```




```{r cor-junk}

mycols=c("blue","darkgreen")
mypch=c(10,18)
names(mycols)=names(mypch)=c("lower_junk","higher_junk")

layout(matrix(c(1:2), 1, 2, byrow = F))
par(mar = c(4, 4, 1, 1))

#create matrix to hold r results
myr=matrix(NA,nrow=2,ncol=2)
rownames(myr)<-c("lp","cp")
colnames(myr)<-c("lower_junk","higher_junk")
mylb=myhb=myn=myp=myr #this is an ugly way of creating 5 matrices

lims=range(c(proportions[,"z_lp"],proportions[,"l_lp"]))

  plot(proportions[,"z_lp"]~proportions[,"l_lp"], xlab=prettynames["l_lp"],ylab=prettynames["z_lp"],
       xlim=lims,ylim=lims,
       col=mycols[proportions$median_junk], pch=mypch[proportions$median_junk])
  lines(c(0,1),c(0,1),lty=2,col="darkgray")

  abline(lm(proportions[,"z_lp"]~proportions[,"l_lp"],subset=c(proportions$median_junk=="lower_junk")),col="blue")

  #get cor properties
  thiscor=cor.test(proportions[proportions$median_junk=="lower_junk","z_lp"],proportions[proportions$median_junk=="lower_junk","l_lp"])
    myr[1,"lower_junk"]=round(thiscor$estimate,3)
    mylb[1,"lower_junk"]=round(thiscor$conf.int[1],3)
    myhb[1,"lower_junk"]=round(thiscor$conf.int[2],3)
    myn[1,"lower_junk"]=round(thiscor$parameter,3)
    myp[1,"lower_junk"]=round(thiscor$p.value,3)
    

  #repeat for LR  
    abline(lm(proportions[,"z_lp"]~proportions[,"l_lp"],subset=c(proportions$median_junk!="lower_junk")),col="darkgreen")

      #get cor properties
  thiscor=cor.test(proportions[proportions$median_junk!="lower_junk","l_lp"],proportions[proportions$median_junk!="lower_junk","z_lp"])
    myr[1,"higher_junk"]=round(thiscor$estimate,3)
    mylb[1,"higher_junk"]=round(thiscor$conf.int[1],3)
    myhb[1,"higher_junk"]=round(thiscor$conf.int[2],3)
    myn[1,"higher_junk"]=round(thiscor$parameter,3)
    myp[1,"higher_junk"]=round(thiscor$p.value,3)
    

    
    # repeat whole for cp
    
    lims=range(c(proportions[,"z_cp"],proportions[,"l_cp"]))

  plot(proportions[,"z_cp"]~proportions[,"l_cp"], xlab=prettynames["l_cp"],ylab=prettynames["z_cp"],
       xlim=lims,ylim=lims,
       col=mycols[proportions$median_junk], pch=mypch[proportions$median_junk])
  lines(c(0,1),c(0,1),lty=2,col="darkgray")

  abline(lm(proportions[,"z_cp"]~proportions[,"l_cp"],subset=c(proportions$median_junk=="lower_junk")),col="blue")

  #get cor properties
  thiscor=cor.test(proportions[proportions$median_junk=="lower_junk","z_cp"],proportions[proportions$median_junk=="lower_junk","l_cp"])
    myr[2,"lower_junk"]=round(thiscor$estimate,3)
    mylb[2,"lower_junk"]=round(thiscor$conf.int[1],3)
    myhb[2,"lower_junk"]=round(thiscor$conf.int[2],3)
    myn[2,"lower_junk"]=round(thiscor$parameter,3)
    myp[2,"lower_junk"]=round(thiscor$p.value,3)
    

  #repeat for higher_junk  
    abline(lm(proportions[,"z_cp"]~proportions[,"l_cp"],subset=c(proportions$median_junk!="lower_junk")),col="darkgreen")

      #get cor properties
  thiscor=cor.test(proportions[proportions$median_junk!="lower_junk","l_cp"],proportions[proportions$median_junk!="lower_junk","z_cp"])
    myr[2,"higher_junk"]=round(thiscor$estimate,3)
    mylb[2,"higher_junk"]=round(thiscor$conf.int[1],3)
    myhb[2,"higher_junk"]=round(thiscor$conf.int[2],3)
    myn[2,"higher_junk"]=round(thiscor$parameter,3)
    myp[2,"higher_junk"]=round(thiscor$p.value,3)
    

```

We investigated this hypothesis by calculating the proportion of their data labeled as "Junk" for each individual child.  There was no significant difference in the proportions of their data labeled as "Junk" for older children with Angelman Syndrome (M = `r round(junkXdiag$estimate["mean in group AngelmanSyndrome"],3)`) compared to the younger low risk children (M = `r round(junkXdiag$estimate["mean in group Low-RiskControl"],3)`): Welch's t(`r junkXdiag$parameter`) = `r round(junkXdiag$statistic,3)`, p = `r round(junkXdiag$p.value,3)`. We therefore collapsed across groups for this exploratory analysis, and split the 20 children using a median split on the proportion of their data labeled as "Junk". Results were similar across these two post-hoc subgroups. For Linguistic Proportion, the correlation across lab and Zooniverse data for the lower junk group was r(`r myn["lp","lower_junk"]`) = `r myr["lp","lower_junk"]`, CI [`r mylb["lp","lower_junk"]`,`r myhb["lp","lower_junk"]`], p=`r myp["lp","lower_junk"]`]; and for the higher junk group it was r(`r myn["lp","higher_junk"]`) = `r myr["lp","higher_junk"]`, CI [`r mylb["lp","higher_junk"]`,`r myhb["lp","higher_junk"]`], p=`r myp["lp","higher_junk"]`. For Canonical Proportion, the correlation across lab and Zooniverse data for the lower junk group was r(`r myn["cp","lower_junk"]`) = `r myr["cp","lower_junk"]`, CI [`r mylb["cp","lower_junk"]`,`r myhb["cp","lower_junk"]`], p=`r myp["cp","lower_junk"]`]; and for the higher junk group it was r(`r myn["cp","higher_junk"]`) = `r myr["cp","higher_junk"]`, CI [`r mylb["cp","higher_junk"]`,`r myhb["cp","higher_junk"]`], p=`r myp["cp","higher_junk"]`. Thus, it does not seem that a higher proportion of "Junk" judgments is an index of low quality data.

<!-- - look at whether kids who have a high "junk" proportion according to Zooniverse  -->
<!--      - have lower levels of concordance across Zoo coders -->


- take all 5 Zoo judgments or 3 or 1 (may need to run the latter two a couple of times to check for potential variance -- if variance is small, we'll just say so, if it's large then we'll have to write a loop and report CIs)
- look at what happens to TD Zoo-PU correlation when you undersample 100 vocs to 80 and 60 and to the same number of non-junk vocs as in AS

